
### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ›´æ–° `README.md`

è¿™ä»½ README æ—¨åœ¨å¸å¼•å¼€å‘è€…ï¼Œè®©ä»–ä»¬åœ¨ 30 ç§’å†…ç†è§£é¡¹ç›®çš„æ ¸å¿ƒä»·å€¼ï¼Œå¹¶åœ¨ 5 åˆ†é’Ÿå†…ä¸Šæ‰‹ä½¿ç”¨å…¶é«˜çº§åŠŸèƒ½ã€‚

```markdown
# LLM Fusion Kit

**ä¸€ä¸ªç»Ÿä¸€ã€å¼ºå¤§ä¸”å¯æ‰©å±•çš„å®¢æˆ·ç«¯ï¼Œç”¨äºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº¤äº’ã€‚**

[![NPM Version](https://img.shields.io/npm/v/llm-kit.svg)](https://www.npmjs.com/package/llm-kit)
[![License](https://img.shields.io/npm/l/llm-kit.svg)](https://github.com/your-username/llm-kit/blob/main/LICENSE)

---

`llm-kit` æ—¨åœ¨è§£å†³ä¸å¤šä¸ª LLM æä¾›å•†äº¤äº’æ—¶çš„å¤æ‚æ€§å’Œä¸ä¸€è‡´æ€§ã€‚å®ƒæä¾›äº†ä¸€ä¸ªç±»ä¼¼ OpenAI SDK çš„ä¼˜é›…æ¥å£ï¼ŒåŒæ—¶åŸç”Ÿæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€å·¥å…·è°ƒç”¨å’Œç®€å•çš„ä»»åŠ¡ç¼–æ’ï¼Œè®©æ‚¨èƒ½ä¸“æ³¨äºæ„å»ºä¸‹ä¸€ä»£ AI åº”ç”¨ï¼Œè€Œä¸æ˜¯å¤„ç†ç¹ççš„ API é€‚é…ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

*   **ç»Ÿä¸€çš„ API**: å­¦ä¹ ä¸€æ¬¡ï¼Œéšå¤„ä½¿ç”¨ã€‚`client.chat.create` æ¥å£ä¸ `openai` åº“é«˜åº¦å…¼å®¹ï¼Œè¿ç§»æˆæœ¬æä½ã€‚
*   **å¤šæä¾›å•†æ”¯æŒ**: å³æ—¶è®¿é—® OpenAI, Google Gemini, DeepSeek, OpenRouter ç­‰ï¼Œè½»æ¾åˆ‡æ¢æ¨¡å‹ä»¥è·å¾—æœ€ä½³æ€§èƒ½å’Œæˆæœ¬ã€‚
*   **å¤šæ¨¡æ€åŸç”Ÿ**: æ— ç¼å¤„ç†æ–‡æœ¬ã€å›¾ç‰‡é™„ä»¶ã€‚å¼ºå¤§çš„æ–‡ä»¶å¤„ç†å™¨å¯åœ¨æµè§ˆå™¨ï¼ˆFile, Blobï¼‰å’Œ Node.jsï¼ˆBuffer, URLï¼‰ç¯å¢ƒä¸­è‡ªåŠ¨è½¬æ¢é™„ä»¶ã€‚
*   **æµå¼å“åº”**: é€šè¿‡ç®€å•çš„ `for await...of` å¾ªç¯ï¼Œè½»æ¾å¤„ç†æµå¼å“åº”ï¼Œæ‰“é€ å®æ—¶äº¤äº’ä½“éªŒã€‚
*   **å·¥å…·è°ƒç”¨ (Function Calling)**: å†…ç½®æ ‡å‡†åŒ–çš„å·¥å…·è°ƒç”¨æ”¯æŒï¼Œä½¿æ‚¨çš„ LLM èƒ½å¤Ÿä¸å¤–éƒ¨ API å’Œå‡½æ•°äº¤äº’ï¼Œæ„å»ºå¼ºå¤§çš„æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ã€‚
*   **ç®€å•ç¼–æ’ (`LLMChain`)**: ä½¿ç”¨æµå¼ API `LLMChain` è½»æ¾æ„å»ºé¡ºåºä»»åŠ¡ï¼Œå°†ä¸€ä¸ª LLM çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªçš„è¾“å…¥ã€‚
*   **é«˜åº¦å¯æ‰©å±•**: é€šè¿‡è‡ªå®šä¹‰ Provider å’Œå¼ºå¤§çš„é’©å­ç³»ç»Ÿ (`beforeRequest`, `afterResponse`, `onError`)ï¼Œè½»æ¾æ‰©å±•å’Œå®šåˆ¶åº“çš„è¡Œä¸ºã€‚
*   **åŒæ„è®¾è®¡**: å¯åœ¨ Node.js å’Œæµè§ˆå™¨ç¯å¢ƒä¸­æ— ç¼è¿è¡Œã€‚

### ğŸ“¦ å®‰è£…

```bash
npm install llm-kit
```

### ğŸš€ å¿«é€Ÿä¸Šæ‰‹

åœ¨å‡ è¡Œä»£ç å†…å¼€å§‹æ‚¨çš„ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ã€‚

```javascript
import { LLMClient } from 'llm-fusion-kit';

const client = new LLMClient({
  provider: 'openai', // or 'gemini', 'deepseek', 'openrouter'
  apiKey: process.env.OPENAI_API_KEY,
});

async function main() {
  try {
    const response = await client.chat.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: 'ä½ å¥½ï¼Œä¸–ç•Œï¼' }],
    });

    console.log(response.choices[0].message.content);
  } catch (error) {
    console.error('è¯·æ±‚å¤±è´¥:', error);
  }
}

main();
```

---

### ğŸŒŸ é«˜çº§ç”¨æ³•

#### 1. å›¾åƒè¾“å…¥ (è§†è§‰èƒ½åŠ›)

å‘é€æ–‡æœ¬å’Œå›¾åƒç»™å¤šæ¨¡æ€æ¨¡å‹ã€‚`llm-fusion-kit` ä¼šè‡ªåŠ¨å¤„ç†ä¸åŒæ¥æºçš„å›¾åƒã€‚

```javascript
import fs from 'fs';

const imageBuffer = fs.readFileSync('./cat.jpg');

const response = await client.chat.create({
  model: 'gemini-1.5-pro-latest', // A model that supports vision
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€‚' },
        { type: 'image_url', image_url: { url: imageBuffer } } // Supports Buffer, File, Blob, URL
      ],
    },
  ],
});

console.log(response.choices[0].message.content);
```

#### 2. æµå¼å“åº”

å®æ—¶è·å–æ¨¡å‹çš„è¾“å‡ºã€‚

```javascript
const stream = await client.chat.create({
  model: 'deepseek-chat',
  messages: [{ role: 'user', content: 'å†™ä¸€é¦–å…³äºä»£ç çš„çŸ­è¯—ã€‚' }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

#### 3. å·¥å…·è°ƒç”¨ (Function Calling)

è®© LLM è°ƒç”¨æ‚¨çš„å‡½æ•°ã€‚

```javascript
// 1. å®šä¹‰ä½ çš„å·¥å…·
const tools = [
  {
    type: 'function',
    function: {
      name: 'get_current_weather',
      description: 'è·å–æŒ‡å®šåœ°ç‚¹çš„å½“å‰å¤©æ°”',
      parameters: {
        type: 'object',
        properties: {
          location: { type: 'string', description: 'åŸå¸‚åï¼Œä¾‹å¦‚ï¼šåŒ—äº¬' },
        },
        required: ['location'],
      },
    },
  },
];

// 2. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œè®©æ¨¡å‹å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
let messages = [{ role: 'user', content: 'åŒ—äº¬ç°åœ¨å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ' }];
const response = await client.chat.create({
  model: 'gpt-4o',
  messages,
  tools,
  tool_choice: 'auto',
});

const message = response.choices[0].message;

// 3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¯·æ±‚è°ƒç”¨å·¥å…·
if (message.tool_calls) {
  messages.push(message); // å°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°å†å²ä¸­
  const toolCall = message.tool_calls[0];
  
  // 4. (åœ¨æ­¤å¤„)æ‰§è¡Œæ‚¨çš„å‡½æ•°
  // const weather = get_current_weather(toolCall.function.arguments);
  const toolResult = JSON.stringify({ temperature: '25Â°C', condition: 'æ™´' });

  // 5. å°†å·¥å…·æ‰§è¡Œç»“æœè¿”å›ç»™æ¨¡å‹
  messages.push({
    role: 'tool',
    tool_call_id: toolCall.id,
    content: toolResult,
  });

  const finalResponse = await client.chat.create({
    model: 'gpt-4o',
    messages,
  });
  console.log(finalResponse.choices[0].message.content); // "åŒ—äº¬ç›®å‰å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ä¸º 25Â°Cã€‚"
}
```

#### 4. ä½¿ç”¨ `LLMChain` ç¼–æ’ä»»åŠ¡

è½»æ¾åœ°å°†å¤šä¸ª LLM è°ƒç”¨ä¸²è”èµ·æ¥ã€‚

```javascript
import { LLMClient, LLMChain } from 'llm-fusion-kit';

const client = new LLMClient({ provider: 'openai', apiKey: '...' });
const chain = new LLMChain(client);

// å®šä¹‰ä¸€ä¸ªä¸¤æ­¥ä»»åŠ¡é“¾
chain
  .add({
    promptTemplate: 'ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦: {topic}',
    inputVariables: ['topic'],
    outputVariable: 'summary',
  })
  .add({
    promptTemplate: 'å°†ä»¥ä¸‹æ‘˜è¦ç¿»è¯‘æˆæ³•è¯­: {summary}',
    inputVariables: ['summary'],
    outputVariable: 'french_summary',
  });

// è¿è¡Œä»»åŠ¡é“¾
const result = await chain.run({ topic: 'äººå·¥æ™ºèƒ½çš„å†å²' });

console.log(result.french_summary);
```

#### 5. ä½¿ç”¨é’©å­

åœ¨è¯·æ±‚ç”Ÿå‘½å‘¨æœŸçš„å…³é”®ç‚¹æ³¨å…¥è‡ªå®šä¹‰é€»è¾‘ï¼Œä¾‹å¦‚æ—¥å¿—è®°å½•æˆ–ç¼“å­˜ã€‚

```javascript
const client = new LLMClient({
  provider: 'openai',
  apiKey: '...',
  hooks: {
    beforeRequest: async (params) => {
      console.log(`[HOOK] å‘é€è¯·æ±‚åˆ°æ¨¡å‹: ${params.model}`);
      return params;
    },
    onError: async (error) => {
      console.error(`[HOOK] è¯·æ±‚å¤±è´¥: ${error.message}`);
    },
  },
});
```

### ğŸ“š API å‚è€ƒ (é«˜çº§)

*   **`new LLMClient(config)`**: åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ã€‚
    *   `config.provider`: `string` - 'openai', 'gemini', etc.
    *   `config.apiKey`: `string`
    *   `config.model`: `string` (å¯é€‰, é»˜è®¤æ¨¡å‹)
    *   `config.hooks`: `object` (å¯é€‰, ç”Ÿå‘½å‘¨æœŸé’©å­)
*   **`client.chat.create(params)`**: å‘èµ·èŠå¤©è¯·æ±‚ã€‚
    *   `params.messages`: `Array<object>`
    *   `params.model`: `string`
    *   `params.stream`: `boolean`
    *   `params.temperature`, `params.max_tokens`, `params.top_p`: `number`
    *   `params.tools`, `params.tool_choice`: `object`
*   **`new LLMChain(client)`**: åˆ›å»ºä»»åŠ¡é“¾å®ä¾‹ã€‚
*   **`chain.add(stepConfig, llmConfig)`**: æ·»åŠ ä¸€ä¸ªæ­¥éª¤ã€‚
*   **`chain.run(initialContext)`**: æ‰§è¡Œä»»åŠ¡é“¾ã€‚

### ğŸ¤ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Request æˆ–åˆ›å»º Issueã€‚

### ğŸ“œ è®¸å¯è¯

[MIT](./LICENSE)
