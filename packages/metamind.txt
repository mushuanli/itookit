// file: src/llm/TopicManager.js

import { ConfigManager } from '../config/configManager.js';
import { generateId } from '../common/utils/utils.js';
import { IContentProvider } from '../sidebar/IContentProvider.js';

// å®šä¹‰ç”¨äºå­˜å‚¨æ‰€æœ‰LLMå¯¹è¯çš„ConfigManageræ¨¡å—å
export const LLM_TOPICS_MODULE = 'llm_topics';

/**
 * @typedef {Object} Session
 * @property {string} id - Session å”¯ä¸€æ ‡è¯†
 * @property {string} title - Session æ ‡é¢˜
 * @property {Array<{role: string, content: string}>} history - å¯¹è¯å†å²
 * @property {number} createdAt - åˆ›å»ºæ—¶é—´æˆ³
 * @property {number} updatedAt - æœ€åæ›´æ–°æ—¶é—´æˆ³
 * @property {object} metadata - é™„åŠ å…ƒæ•°æ®ï¼ˆå¦‚æœ€åä½¿ç”¨çš„ agentï¼‰
 */


/**
 * @class TopicProvider
 * @implements {IContentProvider}
 * @description ä¸€ä¸ªä¸“é—¨ç”¨äºç®¡ç† LLM Topics çš„ Content Providerã€‚
 * å®ƒå®ç°äº† IContentProvider æ¥å£ï¼Œä»¥ä¾¿èƒ½è¢« Sidebar ç­‰é€šç”¨ç»„ä»¶ä½¿ç”¨ï¼Œ
 * åŒæ—¶å°è£…äº† Topic ç‰¹æœ‰çš„ä¸šåŠ¡é€»è¾‘ï¼Œå¦‚è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾ã€‚
 */
export class TopicProvider extends IContentProvider {
    constructor() {
        super();
        // åº•å±‚å­˜å‚¨ä»ç„¶ä¾èµ– ConfigManager
        this.configManager = ConfigManager.instance();
        this.moduleName = LLM_TOPICS_MODULE;

        this._listeners = new Map([['change', []], ['create', []], ['delete', []]]);
        this._configUnsubscribers = [
            this.configManager.on('module:file:set', p => this._forwardEvent(p, 'change')),
            this.configManager.on('module:file:delete', p => this._forwardEvent(p, 'delete'))
        ];
    }

    _forwardEvent(payload, eventName) {
        if (payload.module === this.moduleName) {
            this._emit(eventName, { id: payload.path, data: payload.data });
        }
    }

    /**
     * @override
     * åˆ›å»ºä¸€ä¸ªæ–° Topicã€‚è¿™æ˜¯ createItem çš„ä¸€ä¸ªæ›´å…·ä½“çš„åˆ«åã€‚
     * @param {string} title - Topic çš„æ ‡é¢˜
     * @returns {object} æ–°åˆ›å»ºçš„ Topic å¯¹è±¡ (topicContent)
     */
    createNewTopic(title) { // <--- ç§»é™¤äº† initialPrompt å‚æ•°
        const sanitizedTitle = title.toLowerCase().replace(/[^a-z0-9]+/g, '-').substring(0, 50) || 'new-topic';
        const topicId = generateId('topic');
        const path = `/${sanitizedTitle}-${topicId}.json`;
        
        const topicContent = {
            id: topicId,
            path: path,
            title: title,
            history: [], // <--- **æ ¸å¿ƒä¿®æ”¹ï¼šæ€»æ˜¯åˆ›å»ºä¸€ä¸ªç©ºçš„ history**
            createdAt: Date.now(),
            updatedAt: Date.now(),
        };

        return topicContent;
    }
    
    /**
     * @override
     * IContentProvider çš„ createItem å®ç°ã€‚
     * ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾é€šè¿‡ prompt åˆ›å»ºã€‚
     */
    async createItem({ suggestedPath, type, initialContent }) {
        if (type === 'file') {
            const topic = this.createNewTopic(suggestedPath.split('/').pop() || 'New Topic');
            await this.saveItem(topic.path, { content: topic, tags: [] });
            return { id: topic.path };
        }
        throw new Error("TopicProvider does not support creating directories.");
    }
    
    /**
     * @override
     */
    async saveItem(id, itemData) {
        if (!id || !itemData || !itemData.content) {
            console.error("Invalid data for saveItem:", { id, itemData });
            return;
        }
        
        const topicData = itemData.content;
        topicData.updatedAt = Date.now();
        
        // --- Topic ç‰¹æœ‰çš„ä¸šåŠ¡é€»è¾‘ï¼šè‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾ ---
        const agentIds = [...new Set(topicData.history.map(msg => msg.agentId).filter(Boolean))];
        itemData.tags = agentIds.map(agentId => {
            const agent = this.configManager.getLLMExecutable(agentId);
            return {
                name: agent ? agent.name : agentId,
                color: this.getRandomColor()
            };
        });

        this.configManager.setModuleFile(this.moduleName, id, itemData);
    }

    /**
     * @override
     */
    async getItem(id) {
        return this.configManager.getModuleFile(this.moduleName, id);
    }
    
    /**
     * @override
     */
    async listItems() {
        const allPaths = this.configManager.listModuleFiles(this.moduleName);
        return allPaths.map(path => ({
            id: path,
            path: path,
            name: this._getTitleFromPath(path),
            type: 'file',
            tags: this.configManager.getModuleFile(this.moduleName, path)?.tags || []
        })).sort((a,b) => a.name.localeCompare(b.name));
    }

    _getTitleFromPath(path) {
        try {
            const content = this.configManager.getModuleFile(this.moduleName, path)?.content;
            if (content && content.title) return content.title;
        } catch (e) {}
        return path.split('/').pop().replace('.json', '');
    }

    /**
     * @override
     */
    async deleteItem(id) {
        this.configManager.deleteModuleFile(this.moduleName, id);
    }

    on(eventName, callback) {
        if (!this._listeners.has(eventName)) this._listeners.set(eventName, []);
        this._listeners.get(eventName).push(callback);
        return () => this.off(eventName, callback);
    }

    off(eventName, callback) {
        const listeners = this._listeners.get(eventName);
        if (listeners) {
            const index = listeners.indexOf(callback);
            if (index > -1) listeners.splice(index, 1);
        }
    }

    _emit(eventName, payload) {
        this._listeners.get(eventName)?.forEach(cb => cb(payload));
    }
    
    destroy() {
        this._configUnsubscribers.forEach(unsub => unsub());
        this._listeners.clear();
    }

    getRandomColor() {
        const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3'];
        return colors[Math.floor(Math.random() * colors.length)];
    }
}
/**
 * @file src/llm/core/errors.js
 * @description Defines custom error classes for the library.
 */

export class LLMError extends Error {
    /**
     * Custom error for all library-related issues.
     * @param {string} message - The error message.
     * @param {object} details - Additional error details.
     * @param {Error} [details.cause] - The original error object.
     * @param {string} details.provider - The name of the LLM provider.
     * @param {number} [details.statusCode] - The HTTP status code from the API response.
     */
    constructor(message, { cause, provider, statusCode }) {
        super(message);
        this.name = 'LLMError';
        this.cause = cause;
        this.provider = provider;
        this.statusCode = statusCode;
    }
}/**
 * @file src/llm/chat/plugins/ThinkingPlugin.js
 * @description An optional plugin for ChatEditor to display "thinking process" streams in a collapsible block.
 */

export class ThinkingPlugin {
    /**
     * @param {object} [options={}]
     * @param {boolean} [options.autoExpand=false] - Whether to automatically expand the thinking block.
     */
    constructor(options = {}) {
        this.options = {
            autoExpand: false,
            ...options
        };
        this.name = 'ThinkingPlugin';
        this.thinkingManagers = new Map();
    }

    /**
     * Called by ChatEditor to install the plugin.
     * @param {ChatEditor} chatEditor - The instance of the ChatEditor.
     */
    install(chatEditor) {
        this.chatEditor = chatEditor;

        // Listen for when a new message block's DOM is ready
        this.chatEditor.on('messageBlockAdded', ({ blockId, element }) => {
            // We don't create the UI yet, only when the first thinking chunk arrives.
            // This prepares a manager for each block.
            this.thinkingManagers.set(blockId, new ThinkingUIManager(element, this.options));
        });

        // Listen for thinking chunks for a specific block
        this.chatEditor.on('thinkingChunkReceived', ({ blockId, chunk }) => {
            if (this.thinkingManagers.has(blockId)) {
                this.thinkingManagers.get(blockId).append(chunk);
            }
        });

        // Clean up when a block is removed (important for memory management)
        this.chatEditor.on('messageBlockDestroyed', ({ blockId }) => {
            if (this.thinkingManagers.has(blockId)) {
                this.thinkingManagers.get(blockId).destroy();
                this.thinkingManagers.delete(blockId);
            }
        });
    }

    /**
     * Called when the ChatEditor is destroyed.
     */
    destroy() {
        this.thinkingManagers.forEach(manager => manager.destroy());
        this.thinkingManagers.clear();
        // Unsubscribe from chatEditor events if the `on` method returns an unsubscriber function.
        // For simplicity, we assume ChatEditor handles listener cleanup on its own destroy.
        console.log('ThinkingPlugin destroyed.');
    }
}

/**
 * @private
 * Manages the DOM and state for a single thinking block within a message.
 */
class ThinkingUIManager {
    constructor(blockElement, options) {
        this.blockElement = blockElement;
        this.options = options;
        this.thinkingContent = '';
        this.detailsElement = null;
        this.contentElement = null;
    }

    _ensureElements() {
        if (this.detailsElement) return;

        // Create DOM structure
        this.detailsElement = document.createElement('details');
        this.detailsElement.className = 'chat-thinking-details';
        this.detailsElement.open = this.options.autoExpand;
        this.detailsElement.innerHTML = `
            <summary>
                <span>ğŸ¤” Thinking Process</span>
            </summary>
            <pre class="chat-thinking-details-content"></pre>
        `;

        this.contentElement = this.detailsElement.querySelector('.chat-thinking-details-content');
        this.contentElement.textContent = this.thinkingContent;

        // Append to the message block's main content area
        const renderContent = this.blockElement.querySelector('.chat-message-render-content');
        if (renderContent) {
            renderContent.appendChild(this.detailsElement);
        }

        this._addControls();
    }

    append(chunk) {
        this._ensureElements();
        this.thinkingContent += chunk;
        this.contentElement.textContent = this.thinkingContent;
        // Auto-scroll the thinking pre block
        this.contentElement.scrollTop = this.contentElement.scrollHeight;
    }

    _addControls() {
        const summary = this.detailsElement.querySelector('summary');
        const controls = document.createElement('span');
        controls.className = 'chat-thinking-controls';
        controls.innerHTML = `
            <button class="chat-thinking-control-btn" data-action="copy" title="Copy thinking process">
                <i class="fas fa-copy"></i>
            </button>
        `;
        
        summary.appendChild(controls);

        const copyBtn = controls.querySelector('[data-action="copy"]');
        copyBtn.addEventListener('click', this._handleCopyClick);
    }
    
    _handleCopyClick = async (e) => {
        e.stopPropagation(); // Prevent details toggling
        e.preventDefault();
        
        const button = e.currentTarget;
        try {
            await navigator.clipboard.writeText(this.thinkingContent);
            button.innerHTML = '<i class="fas fa-check"></i>';
            setTimeout(() => {
                button.innerHTML = '<i class="fas fa-copy"></i>';
            }, 2000);
        } catch (err) {
            console.error('Failed to copy thinking process:', err);
        }
    };


    destroy() {
        // Remove event listeners to prevent memory leaks
        const copyBtn = this.detailsElement?.querySelector('[data-action="copy"]');
        if (copyBtn) {
            copyBtn.removeEventListener('click', this._handleCopyClick);
        }
    }
}/**
 * @file src/llm/executor/ExecutorFactory.js
 * @description æ ¹æ® Executable çš„ç±»å‹åˆ›å»ºå…·ä½“æ‰§è¡Œå™¨å®ä¾‹çš„å·¥å‚ç±»
 */

import { ConfigManager } from '../../config/configManager.js';
import { AgentExecutor } from './AgentExecutor.js';
import { AgentGroupExecutor } from './AgentGroupExecutor.js';
import { IExecutor } from './IExecutor.js';

/**
 * @class ExecutorFactory
 * @description å·¥å‚ç±»ï¼Œè´Ÿè´£æ ¹æ® Executable ID åˆ›å»ºå¯¹åº”çš„æ‰§è¡Œå™¨å®ä¾‹
 * 
 * æ”¯æŒçš„ç±»å‹ï¼š
 * - 'agent': åˆ›å»º AgentExecutor
 * - 'group': åˆ›å»º AgentGroupExecutor
 */
export class ExecutorFactory {
    /**
     * æ ¹æ® Executable ID åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª IExecutor å®ä¾‹
     * 
     * @param {string} executableId - Executable çš„ ID
     * @returns {import('./IExecutor.js').IExecutor} æ‰§è¡Œå™¨å®ä¾‹
     * @throws {Error} å¦‚æœ Executable ä¸å­˜åœ¨æˆ–ç±»å‹æœªçŸ¥
     * 
     * @example
     * const executor = ExecutorFactory.create('my-agent-id');
     * const result = await executor.run('ç”¨æˆ·è¾“å…¥', [], {});
     */
    static create(executableId) {
        const configManager = ConfigManager.instance();
        const executable = configManager.getLLMExecutable(executableId); 

        if (!executable) {
            throw new Error(`Executable with ID '${executableId}' not found.`);
        }

        switch (executable.type) {
            case 'agent':
                return new AgentExecutor(executableId);
            case 'group':
                return new AgentGroupExecutor(executableId);
            default:
                throw new Error(`Unknown executable type: '${executable.type}' for ID '${executableId}'`);
        }
    }
}
/**
 * @file src/llm/executor/AgentExecutor.js
 * @description å•ä¸ª Agent çš„æ‰§è¡Œå™¨ï¼Œè´Ÿè´£å¤„ç†æç¤ºè¯æ¨¡æ¿ã€è‡ªåŠ¨æ­¥éª¤å’Œ LLM è°ƒç”¨
 */

import { IExecutor } from './IExecutor.js';
import { ConfigManager } from '../../config/configManager.js';
import { LLMService } from '../core/LLMService.js';

/**
 * @class AgentExecutor
 * @extends IExecutor
 * @description æ‰§è¡Œå•ä¸ª Agent çš„é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
 * - æç¤ºè¯æ¨¡æ¿å˜é‡æ›¿æ¢
 * - å¤šæ­¥éª¤è‡ªåŠ¨æç¤ºè¯æ‰§è¡Œ
 * - å†å²è®°å½•ç®¡ç†
 * - æµå¼å“åº”å¤„ç†
 */
export class AgentExecutor extends IExecutor {
    /**
     * @param {string} agentId - è¦æ‰§è¡Œçš„ Agent çš„ ID
     * @throws {Error} å¦‚æœ Agent ä¸å­˜åœ¨æˆ–ç±»å‹ä¸æ­£ç¡®
     */
    constructor(agentId) {
        super();
        this.agentId = agentId;
        this.configManager = ConfigManager.instance();
        this.llmService = LLMService.getInstance();
        
        // +++ ä¿®æ”¹ä¸ºä½¿ç”¨æ–°çš„ç»Ÿä¸€æ–¹æ³•å’Œæ•°æ®ç»“æ„ +++
        this.executable = this.configManager.getLLMExecutable(this.agentId);
        if (!this.executable || this.executable.type !== 'agent') {
            throw new Error(`AgentExecutable with ID '${agentId}' not found or is not an agent type.`);
        }
        
        this.client = this.llmService.getClient(this.executable.config.connectionId);
    }

    /**
     * æ‰§è¡Œ Agent
     * 
     * @param {string} userPrompt - ç”¨æˆ·çš„åˆå§‹è¾“å…¥
     * @param {Array<import('./IExecutor.js').MessageHistoryItem>} [externalHistory=[]] - å¤–éƒ¨ä¼ å…¥çš„å¯¹è¯å†å²
     * @param {Object} [initialVariables={}] - åˆå§‹å˜é‡ï¼Œç”¨äºæ¨¡æ¿æ›¿æ¢
     * @param {import('./IExecutor.js').ExecutorCallbacks} [callbacks={}] - ç”¨äºæµå¼å“åº”å’Œè¿›åº¦çš„å›è°ƒ
     * @returns {Promise<import('./IExecutor.js').ExecutorResult>}
     */
    async run(userPrompt, externalHistory = [], initialVariables = {}, callbacks = {}) {
        // âœ… é˜²å¾¡æ€§ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿ userPrompt æ˜¯å­—ç¬¦ä¸²
        if (typeof userPrompt !== 'string') {
            console.error('[AgentExecutor] userPrompt must be a string, received:', {
                type: typeof userPrompt,
                value: userPrompt,
                agentId: this.agentId
            });
            
            // å°è¯•æ™ºèƒ½è½¬æ¢
            if (userPrompt && typeof userPrompt === 'object') {
                // å¦‚æœæ˜¯ group response å¯¹è±¡ï¼Œæå–æ–‡æœ¬æ‘˜è¦
                if (userPrompt.type === 'group_response') {
                    userPrompt = userPrompt.responses
                        .map(r => `[${r.agentId}]: ${r.content}`)
                        .join('\n\n');
                    console.warn('[AgentExecutor] Converted group_response object to text');
                } else {
                    // å…¶ä»–å¯¹è±¡ï¼Œè½¬ä¸º JSON
                    userPrompt = JSON.stringify(userPrompt, null, 2);
                    console.warn('[AgentExecutor] Converted object to JSON string');
                }
            } else {
                // å…œåº•è½¬æ¢
                userPrompt = String(userPrompt);
            }
        }

        const { config } = this.executable;
        const displayName = `${this.executable.icon || 'ğŸ¤–'} ${this.executable.name}`;
        
        // âœ… éªŒè¯å’Œæ¸…ç† autoPrompts
        const autoPrompts = (config.autoPrompts || [])
            .filter(prompt => prompt != null)
            .map((prompt, index) => {
                if (typeof prompt !== 'string') {
                    console.warn(`[AgentExecutor] Auto prompt[${index}] is not a string:`, typeof prompt);
                    return typeof prompt === 'object' ? JSON.stringify(prompt, null, 2) : String(prompt);
                }
                return prompt;
            });
        
        const allPrompts = [userPrompt, ...autoPrompts];
        const maxHistoryLength = config.maxHistoryLength ?? -1;

        // æ·±æ‹·è´å†å²è®°å½•ï¼Œé¿å…ä¿®æ”¹å¤–éƒ¨æ•°æ®
        const history = JSON.parse(JSON.stringify(externalHistory));
        
        // å¦‚æœ history ä¸ºç©ºä¸”æœ‰ system promptï¼Œæ·»åŠ è¿›å»
        if (history.length === 0 && config.systemPrompt) {
            history.push({ role: 'system', content: config.systemPrompt });
        }

        let context = { ...initialVariables, user_prompt: userPrompt };
        let finalResponse = '';

        try {
            for (let i = 0; i < allPrompts.length; i++) {
                const isAutoStep = i > 0;
                const currentPromptTemplate = allPrompts[i];
                
                // æ‰§è¡Œå˜é‡æ›¿æ¢
                const formattedTemplate = this._formatPrompt(currentPromptTemplate, context);
                
                // è§£ææ ‡é¢˜å’Œå†…å®¹
                const { title, content: finalPrompt } = this._parsePrompt(formattedTemplate);
                
                // è§¦å‘æ­¥éª¤å¼€å§‹å›è°ƒ
                callbacks.onStepStart?.(i, { title, prompt: formattedTemplate }, isAutoStep);

                // å°†å¤„ç†åçš„æç¤ºè¯æ·»åŠ åˆ°å†å²è®°å½•
                history.push({ role: 'user', content: finalPrompt });
                
                // è§¦å‘å“åº”å¼€å§‹å›è°ƒ
                callbacks.onResponseStart?.(i, this.agentId, displayName);

                // åº”ç”¨å†å²æ¶ˆæ¯é™åˆ¶
                const messagesToSend = this._limitHistory(history, maxHistoryLength);

                // è°ƒç”¨ LLM API
                const stream = await this.client.chat.create({
                    model: config.modelName,
                    messages: messagesToSend,
                    stream: true,
                    temperature: config.temperature,
                });
                
                // å¤„ç†æµå¼å“åº”
                let fullStepResponse = '';
                for await (const chunk of stream) {
                    const content = chunk.choices[0]?.delta?.content || '';
                    if (content) {
                        fullStepResponse += content;
                        callbacks.onChunk?.(content);
                    }
                }
                
                // âœ… å°†å“åº”æ·»åŠ åˆ°å†å²è®°å½•ï¼ŒåŒ…å« agentId
                history.push({ 
                    role: 'assistant', 
                    content: fullStepResponse, 
                    agentId: this.agentId
                });
                
                finalResponse = fullStepResponse;
                
                // æ›´æ–°ä¸Šä¸‹æ–‡å˜é‡ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
                context[`step_${i}_response`] = fullStepResponse;
                context['last_response'] = fullStepResponse;

                // è§¦å‘æ­¥éª¤ç»“æŸå›è°ƒ
                callbacks.onStepEnd?.(i, fullStepResponse, this.agentId);
            }
        } catch (error) {
            console.error(`[AgentExecutor] Error executing agent '${this.agentId}':`, error);
            callbacks.onError?.(error, this.agentId);
            return { success: false, history, finalResponse: '' };
        }

        return { success: true, history, finalResponse };
    }

    /**
     * é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡
     * 
     * @private
     * @param {Array<import('./IExecutor.js').MessageHistoryItem>} history - å®Œæ•´çš„æ¶ˆæ¯å†å²
     * @param {number} maxHistoryLength - æœ€å¤§å†å²æ¶ˆæ¯æ¡æ•°ï¼ˆ-1 è¡¨ç¤ºä¸é™åˆ¶ï¼‰
     * @returns {Array<import('./IExecutor.js').MessageHistoryItem>} å¤„ç†åçš„æ¶ˆæ¯å†å²
     */
    _limitHistory(history, maxHistoryLength) {
        // -1 è¡¨ç¤ºä¸é™åˆ¶ï¼Œç›´æ¥è¿”å›å…¨éƒ¨å†å²
        if (maxHistoryLength === -1) {
            return history;
        }
        
        // 0 è¡¨ç¤ºä¸å‘é€å†å²ï¼Œåªä¿ç•™ system promptï¼ˆå¦‚æœæœ‰ï¼‰å’Œæœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        if (maxHistoryLength === 0) {
            const systemMessage = history.find(msg => msg.role === 'system');
            const lastUserMessage = [...history].reverse().find(msg => msg.role === 'user');
            
            return [
                ...(systemMessage ? [systemMessage] : []),
                ...(lastUserMessage ? [lastUserMessage] : [])
            ];
        }
        
        // å¤§äº 0ï¼šä¿ç•™ system prompt + æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
        const systemMessage = history.find(msg => msg.role === 'system');
        const conversationMessages = history.filter(msg => msg.role !== 'system');
        
        // è®¡ç®—è¦ä¿ç•™çš„æ¶ˆæ¯èŒƒå›´
        const startIndex = Math.max(0, conversationMessages.length - maxHistoryLength);
        const recentMessages = conversationMessages.slice(startIndex);
        
        return [
            ...(systemMessage ? [systemMessage] : []),
            ...recentMessages
        ];
    }

    /**
     * æ ¼å¼åŒ–æç¤ºè¯æ¨¡æ¿ï¼Œæ›¿æ¢å˜é‡å ä½ç¬¦
     * 
     * @private
     * @param {string} template - æç¤ºè¯æ¨¡æ¿
     * @param {Object} variables - å˜é‡é”®å€¼å¯¹
     * @returns {string} æ ¼å¼åŒ–åçš„æç¤ºè¯
     */
    _formatPrompt(template, variables) {
        // âœ… æœ€åçš„é˜²çº¿ï¼šç±»å‹æ£€æŸ¥
        if (template == null) {
            console.warn('[AgentExecutor] Received null/undefined template');
            return '';
        }
        
        if (typeof template !== 'string') {
            console.error('[AgentExecutor] CRITICAL: template is not a string after validation:', {
                type: typeof template,
                value: template,
                agentId: this.agentId
            });
            template = String(template);
        }
        
        // æ›¿æ¢ {variable} æ ¼å¼çš„å ä½ç¬¦
        return template.replace(/{(\w+)}/g, (_, key) => {
            return variables[key] !== undefined ? variables[key] : `{${key}}`;
        });
    }

    /**
     * è§£ææç¤ºè¯ï¼Œåˆ†ç¦»å‡ºæ ‡é¢˜å’Œæ­£æ–‡
     * å¦‚æœç¬¬ä¸€è¡Œæ˜¯ Markdown æ ‡é¢˜ï¼ˆä»¥ # å¼€å¤´ï¼‰ï¼Œåˆ™ä½œä¸ºæ ‡é¢˜ï¼Œå…¶ä½™ä½œä¸ºå†…å®¹
     * 
     * @private
     * @param {string} rawPrompt - åŸå§‹æç¤ºè¯
     * @returns {{title: string, content: string}}
     */
    _parsePrompt(rawPrompt) {
        const lines = rawPrompt.split('\n');
        if (lines[0].startsWith('# ')) {
            return {
                title: lines[0].substring(2).trim(),
                content: lines.slice(1).join('\n').trim()
            };
        }
        return {
            title: `Auto Step: ${rawPrompt.substring(0, 30)}...`,
            content: rawPrompt
        };
    }
}
/**
 * @file src/llm/chat/index.js
 * @description Chat ç¼–è¾‘å™¨ï¼Œæ”¯æŒå• Agent å’Œ Groupï¼ˆä¸²è¡Œ/å¹¶è¡Œï¼‰æ‰§è¡Œ
 * 
 * æ ¸å¿ƒåŠŸèƒ½ï¼š
 * - æ”¯æŒå•ä¸ª Agent æ‰§è¡Œ
 * - æ”¯æŒ Agent Group ä¸²è¡Œæ‰§è¡Œï¼ˆå‰ä¸€ä¸ªè¾“å‡ºä½œä¸ºåä¸€ä¸ªè¾“å…¥ï¼‰
 * - æ”¯æŒ Agent Group å¹¶è¡Œæ‰§è¡Œï¼ˆåŒæ—¶è¿è¡Œå¤šä¸ª Agentï¼‰
 * - ç®¡ç†å¯¹è¯å†å²ï¼ˆTopicï¼‰
 * - å®æ—¶æµå¼å“åº”æ˜¾ç¤º
 * - è‡ªåŠ¨ä¿å­˜å¯¹è¯è®°å½•
 */

import './styles.css';
import { IEditor } from '../../common/interfaces/IEditor.js';
import { BaseEditor } from '../../editor/baseEditor.js';
import { ConfigManager } from '../../config/configManager.js';
import { generateId } from '../../common/utils/utils.js';
import { ExecutorFactory } from '../executor/ExecutorFactory.js';
import { TopicProvider } from '../TopicProvider.js'; // New import

/**
 * @typedef {Object} MessageBlock
 * @property {BaseEditor} editor - BaseEditor å®ä¾‹
 * @property {string} blockId - æ¶ˆæ¯å—çš„å”¯ä¸€ ID
 * @property {boolean} [isFirstChunk] - æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªæµå¼æ•°æ®å—ï¼ˆç”¨äºå¹¶è¡Œæ¨¡å¼ï¼‰
 */

/**
 * @typedef {Object} TopicContext
 * @property {string} [preferredAgentId] - ä¼˜å…ˆä½¿ç”¨çš„ Agent ID
 */

/**
 * @enum {string}
 * @description å®šä¹‰ ChatEditor çš„æ ¸å¿ƒæ‰§è¡ŒçŠ¶æ€
 */
const ExecutionState = {
    IDLE: 'idle',
    EXECUTING_SINGLE: 'executing_single', // å• Agent æˆ–ä¸²è¡Œ Group
    EXECUTING_PARALLEL: 'executing_parallel', // å¹¶è¡Œ Group
};


/**
 * @class ChatEditor
 * @extends IEditor
 * @description 
 * Chat ç¼–è¾‘å™¨ç±»ï¼Œå®ç°äº† IEditor æ¥å£ã€‚
 * 
 * æ‰§è¡Œæ¨¡å¼ï¼š
 * - **Agent æ¨¡å¼**ï¼šå•ä¸ª Agent æ‰§è¡Œï¼Œæ”¯æŒå¤šæ­¥éª¤è‡ªåŠ¨æç¤ºè¯
 * - **Serial æ¨¡å¼**ï¼šå¤šä¸ª Agent ä¸²è¡Œæ‰§è¡Œï¼Œå‰ä¸€ä¸ªçš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªçš„è¾“å…¥
 * - **Parallel æ¨¡å¼**ï¼šå¤šä¸ª Agent å¹¶è¡Œæ‰§è¡Œï¼ŒåŒæ—¶è¿è¡Œå¹¶æ˜¾ç¤ºå„è‡ªçš„å“åº”
 * 
 * @example
 * const container = document.getElementById('chat-container');
 * const editor = new ChatEditor(container, {
 *   showSidebarToggle: true
 * });
 * 
 * // åŠ è½½ä¸€ä¸ª Topic
 * editor.loadTopic(myTopic, { preferredAgentId: 'my-agent-id' });
 */
export class ChatEditor extends IEditor {
    /**
     * åˆ›å»ºä¸€ä¸ªæ–°çš„ ChatEditor å®ä¾‹
     * 
     * @param {HTMLElement} container - ç¼–è¾‘å™¨çš„å®¹å™¨å…ƒç´ 
     * @param {Object} [options={}] - é…ç½®é€‰é¡¹
     * @param {boolean} [options.showSidebarToggle=true] - æ˜¯å¦æ˜¾ç¤ºä¾§è¾¹æ åˆ‡æ¢æŒ‰é’®
     */
    constructor(container, options = {}) {
        super(container, options);

        this.container = container;
        this.options = { showSidebarToggle: true, ...options };
        
        // --- æ ¸å¿ƒä¿®æ”¹ï¼šä¾èµ–æ³¨å…¥å’ŒçŠ¶æ€åˆå§‹åŒ– ---
        if (!options.topicProvider) {
            throw new Error("ChatEditor requires a 'topicProvider' in its options.");
        }
        this.topicProvider = options.topicProvider;
        this.configManager = ConfigManager.instance(); // ä»ç„¶éœ€è¦ç”¨äºè·å– Agent ä¿¡æ¯
        this.providerUnsubscribers = [];

        this.currentTopic = null;
        this.activeTopicId = null;
        // --- ä¿®æ”¹ç»“æŸ ---

        this._listeners = new Map();
        this.messageBlocks = new Map();
        this.currentResponseBlock = null;
        this.parallelResponseBlocks = new Map();

        // UI å…ƒç´ å¼•ç”¨
        this.welcomeArea = null; // --- NEW ---
        this.inputBar = null;
        this.agentSelect = null;
        this.promptInput = null;
        this.sendButton = null;
        this.charCount = null;
        this.mainArea = null;

        // çŠ¶æ€ç®¡ç†
        this.currentTopic = null;
        this.selectedAgentId = null;

        /**
         * æ ¸å¿ƒæ‰§è¡ŒçŠ¶æ€æœº
         * @type {ExecutionState}
         * @private
         */
        this.executionState = ExecutionState.IDLE;

        /**
         * æ ‡è®°æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªæµå¼æ•°æ®å—ï¼ˆç”¨äºä¸²è¡Œ/å• Agent æ¨¡å¼ï¼‰
         * @type {boolean}
         * @private
         */
        this._isFirstChunk = true;

        this._setupDOM();
        this._loadAgents();
        this._bindEvents();
        this._bindProviderEvents(); // ç»‘å®š Provider äº‹ä»¶ä»¥å¤„ç†åˆ é™¤å†²çª
    }

    /**
     * è®¾ç½®ç¼–è¾‘å™¨çš„ DOM ç»“æ„
     * @private
     */
    _setupDOM() {
        this.container.innerHTML = `
            <div class="chat-editor-wrapper">
                <div class="chat-editor-welcome">
                    <div style="padding: 20px; text-align: center; color: #888; height: 100%; display: flex; align-items: center; justify-content: center; flex-direction: column;">
                        <h2>Welcome to LLM Chat</h2>
                        <p>Select a conversation from the sidebar or start a new one.</p>
                    </div>
                </div>
                <div class="chat-editor-main"></div>
                <div class="chat-input-bar">
                    <select class="chat-agent-select">
                        <option value="">é€‰æ‹© Agent æˆ– Group...</option>
                    </select>
                    <div class="chat-input-wrapper">
                        <textarea class="chat-prompt-input" rows="1" 
                                  placeholder="è¾“å…¥æ¶ˆæ¯... (Shift+Enter æ¢è¡Œ)"></textarea>
                        <span class="char-count">0</span>
                    </div>
                    <button class="chat-send-btn">å‘é€</button>
                </div>
            </div>
        `;

        this.welcomeArea = this.container.querySelector('.chat-editor-welcome');
        this.mainArea = this.container.querySelector('.chat-editor-main');
        this.inputBar = this.container.querySelector('.chat-input-bar');
        this.agentSelect = this.container.querySelector('.chat-agent-select');
        this.promptInput = this.container.querySelector('.chat-prompt-input');
        this.sendButton = this.container.querySelector('.chat-send-btn');
        this.charCount = this.container.querySelector('.char-count');

        // Initially, hide the chat interface and show the welcome message
        this.mainArea.style.display = 'none';
        this.inputBar.style.display = 'none';
        this.welcomeArea.style.display = 'flex';
    }

    /**
     * --- æ–°å¢ï¼šç»‘å®š Provider äº‹ä»¶ ---
     * @private
     */
    _bindProviderEvents() {
        const unsubDelete = this.topicProvider.on('delete', (payload) => {
            if (payload.id === this.activeTopicId) {
                console.warn(`Active topic ${this.activeTopicId} was deleted. Clearing ChatEditor state.`);
                this._clearState();
            }
        });
        this.providerUnsubscribers.push(unsubDelete);
    }
    
    /**
     * --- æ–°å¢ï¼šæ¸…ç†çŠ¶æ€çš„æ–¹æ³• ---
     * @private
     */
    _clearState() {
        this.currentTopic = null;
        this.activeTopicId = null;

        // Hide chat UI and show a message in the welcome area
        this.mainArea.style.display = 'none';
        this.inputBar.style.display = 'none';
        this.welcomeArea.innerHTML = `<div class="chat-editor-deleted-message" style="padding: 20px; text-align: center; color: #888;">This conversation has been deleted or is unavailable.</div>`;
        this.welcomeArea.style.display = 'flex';

        this.messageBlocks.forEach(editor => editor.destroy());
        this.messageBlocks.clear();
        this.promptInput.disabled = true;
        this.sendButton.disabled = true;
        this.agentSelect.disabled = true;
    }

    /**
     * --- API å˜æ›´ï¼šä» loadTopic(topic) å˜ä¸º loadTopicById(topicId) ---
     * @param {string} topicId - è¦åŠ è½½çš„ Topic çš„å”¯ä¸€ID (å³å…¶è·¯å¾„)ã€‚
     * @param {object} [context={}] - å¤–éƒ¨ä¼ å…¥çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚ preferredAgentIdã€‚
     */
    async loadTopicById(topicId, context = {}) {
        this.activeTopicId = topicId;
        try {
            const itemData = await this.topicProvider.getItem(topicId);
            if (!itemData || !itemData.content) {
                throw new Error(`Topic with id ${topicId} not found.`);
            }

            // Show the chat interface and hide the welcome screen
            this.welcomeArea.style.display = 'none';
            this.mainArea.style.display = 'block';
            this.inputBar.style.display = 'flex';

            const topic = itemData.content;
            this.currentTopic = topic;

            // --- ä»¥ä¸‹ä¸ºåŸ loadTopic çš„æ¸²æŸ“é€»è¾‘ ---
            this.promptInput.disabled = false;
            this.sendButton.disabled = false;
            this.agentSelect.disabled = false;

            this.mainArea.innerHTML = '';
            this.messageBlocks.forEach(editor => editor.destroy());
            this.messageBlocks.clear();

            topic.history.forEach(msg => {
                if (msg.role === 'system' && msg._isMetadata) return; // Hide metadata system messages
                if (msg.role === 'system') return; // Hide regular system messages for now
                let role = msg.role === 'user' ? 'User' : 'Assistant';
                if (msg.role === 'assistant') {
                    const agent = this.configManager.getLLMExecutable(msg.agentId);
                    role = agent ? `${agent.icon || 'ğŸ¤–'} ${agent.name}` : 'Assistant';
                }
                if (msg._structuredContent?.type === 'group_response') {
                    this._renderGroupResponse(msg._structuredContent);
                } else {
                    this._addMessageBlock(role, msg.content);
                }
            });
            
            // æ™ºèƒ½é€‰æ‹© Agent çš„é€»è¾‘ä¿æŒä¸å˜
            this._selectDefaultAgent(topic, context);
            
            this.promptInput.focus();

        } catch (error) {
            console.error(`Failed to load topic ${topicId}:`, error);
            this._clearState();
        }
    }

    /**
     * æ¸²æŸ“ Group çš„ç»“æ„åŒ–å“åº”ï¼ˆç‰¹åˆ«æ˜¯å¹¶è¡Œæ¨¡å¼ï¼‰
     * 
     * @param {Object} structuredContent - ç»“æ„åŒ–å“åº”å†…å®¹
     * @param {string} structuredContent.type - å“åº”ç±»å‹ï¼ˆ'group_response'ï¼‰
     * @param {string} structuredContent.mode - æ‰§è¡Œæ¨¡å¼ï¼ˆ'parallel' | 'serial'ï¼‰
     * @param {Array<Object>} structuredContent.responses - æ‰€æœ‰ Agent çš„å“åº”
     * @private
     */
    _renderGroupResponse(structuredContent) {
        if (structuredContent.mode === 'parallel') {
            const blockWrapper = document.createElement('div');
            blockWrapper.className = 'chat-message-block chat-group-parallel';
            
            // âœ… è·å– Group çš„åç§°
            const groupName = this._getGroupDisplayName(structuredContent);
            
            const groupHeader = document.createElement('div');
            groupHeader.className = 'chat-group-header';
            groupHeader.innerHTML = `<span class="group-icon">ğŸ‘¥</span> ${groupName} (${structuredContent.responses.length} agents)`;
            blockWrapper.appendChild(groupHeader);
            
            const responsesContainer = document.createElement('div');
            responsesContainer.className = 'chat-parallel-responses';
            
            structuredContent.responses.forEach(response => {
                const agent = this.configManager.getLLMExecutable(response.agentId);
                const agentName = agent ? `${agent.icon || 'ğŸ¤–'} ${agent.name}` : response.agentId;
                
                const responseBlock = document.createElement('div');
                responseBlock.className = 'chat-parallel-response-item';
                
                const agentHeader = document.createElement('div');
                agentHeader.className = 'parallel-agent-header';
                agentHeader.textContent = agentName;
                
                const contentPre = document.createElement('pre');
                contentPre.className = 'parallel-agent-content';
                contentPre.textContent = response.content;
                
                responseBlock.appendChild(agentHeader);
                responseBlock.appendChild(contentPre);
                responsesContainer.appendChild(responseBlock);
            });
            
            blockWrapper.appendChild(responsesContainer);
            this.mainArea.appendChild(blockWrapper);
            blockWrapper.scrollIntoView({ behavior: 'smooth', block: 'end' });
        }
    }

    /**
     * è·å– Group çš„æ˜¾ç¤ºåç§°
     * 
     * @param {Object} structuredContent - ç»“æ„åŒ–å“åº”å†…å®¹
     * @returns {string} Group æ˜¾ç¤ºåç§°
     * @private
     */
    _getGroupDisplayName(structuredContent) {
        // å°è¯•ä»ç¬¬ä¸€ä¸ªå“åº”çš„ agentId å‘ä¸ŠæŸ¥æ‰¾çˆ¶ Group
        if (structuredContent.responses && structuredContent.responses.length > 0) {
            const firstAgentId = structuredContent.responses[0].agentId;
            // æŸ¥æ‰¾åŒ…å«è¿™ä¸ª agent çš„ group
            const allGroups = this.configManager.getAllLLMExecutables().filter(e => e.type === 'group');
            for (const group of allGroups) {
                if (group.children && group.children.includes(firstAgentId)) {
                    return `${group.icon || 'ğŸ‘¥'} ${group.name}`;
                }
            }
        }
        return 'Parallel Group';
    }

    /**
     * åŠ¨æ€åˆ›å»ºä¸€ä¸ªæ¶ˆæ¯å—ï¼ˆä½¿ç”¨ BaseEditorï¼‰
     * 
     * @param {string} role - æ¶ˆæ¯è§’è‰²ï¼ˆå¦‚ 'User', 'Assistant', Agent åç§°ç­‰ï¼‰
     * @param {string} [content=''] - æ¶ˆæ¯å†…å®¹
     * @param {string|null} [customTitle=null] - è‡ªå®šä¹‰æ ‡é¢˜ï¼ˆå¦‚æœä¸æä¾›ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
     * @returns {BaseEditor} åˆ›å»ºçš„ BaseEditor å®ä¾‹
     * @private
     */
    _addMessageBlock(role, content = '', customTitle = null) {
        const blockId = generateId('msg-block');
        const blockWrapper = document.createElement('div');
        blockWrapper.className = 'chat-message-block';
        blockWrapper.id = blockId;
        
        this.mainArea.appendChild(blockWrapper);

        const editor = new BaseEditor(blockWrapper, {
            ...this.options,
            readOnly: true,
            showSidebarToggle: this.messageBlocks.size === 0 && this.options.showSidebarToggle,
            autoHeight: true,
            minHeight: 100,
            maxHeight: 600,
        });
        
        if (this.messageBlocks.size === 0) {
            editor.on('toggle-sidebar-request', () => this._emit('toggle-sidebar-request'));
        }

        const cleanContent = this._cleanMarkdownForTitle(content);
        const titlePrefix = cleanContent.substring(0, 40);
        const defaultTitle = role 
            ? `${role}: ${titlePrefix}${cleanContent.length > 40 ? '...' : ''}`
            : titlePrefix + (cleanContent.length > 40 ? '...' : '');
        
        editor.setTitle(customTitle || defaultTitle);
        editor.setText(content);
        
        this.messageBlocks.set(blockId, editor);
        blockWrapper.scrollIntoView({ behavior: 'smooth', block: 'end' });
        
        return editor;
    }

    /**
     * ä¸ºå¹¶è¡Œæ¨¡å¼åˆ›å»ºç‹¬ç«‹çš„å“åº”å—
     * 
     * @param {string} agentId - Agent çš„ ID
     * @param {string} agentDisplayName - Agent çš„æ˜¾ç¤ºåç§°
     * @returns {BaseEditor} åˆ›å»ºçš„ BaseEditor å®ä¾‹
     * @private
     */
    _createParallelResponseBlock(agentId, agentDisplayName) {
        const blockId = generateId('parallel-block');
        const blockWrapper = document.createElement('div');
        blockWrapper.className = 'chat-message-block chat-parallel-block';
        blockWrapper.id = blockId;
        
        this.mainArea.appendChild(blockWrapper);

        const editor = new BaseEditor(blockWrapper, {
            ...this.options,
            readOnly: true,
            showSidebarToggle: false,
            autoHeight: true,
            minHeight: 100,
            maxHeight: 600,
        });
        
        editor.setTitle(`${agentDisplayName} is typing...`);
        editor.setText('');
        
        this.messageBlocks.set(blockId, editor);
        this.parallelResponseBlocks.set(agentId, { editor, blockId, isFirstChunk: true });
        
        blockWrapper.scrollIntoView({ behavior: 'smooth', block: 'end' });
        
        return editor;
    }

    /**
     * åˆ›å»ºä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯å—ï¼ˆç”¨äºæ˜¾ç¤ºè‡ªåŠ¨æç¤ºè¯ç­‰ä¿¡æ¯ï¼‰
     * 
     * @param {string} title - æ ‡é¢˜ï¼Œä¾‹å¦‚ "Auto Step: Extract Keywords"
     * @param {string} content - è¦æ˜¾ç¤ºçš„å†…å®¹
     * @private
     */
    _addSystemMessageBlock(title, content) {
        const blockWrapper = document.createElement('div');
        blockWrapper.className = 'chat-message-block system-message'; 
        
        blockWrapper.innerHTML = `
            <div class="system-message-header">
                <span class="system-message-icon">âš™ï¸</span>
                <span class="system-message-title">${title}</span>
            </div>
            <pre class="system-message-content">${content}</pre>
        `;
        
        this.mainArea.appendChild(blockWrapper);
        blockWrapper.scrollIntoView({ behavior: 'smooth', block: 'end' });
    }

    /**
     * åŠ è½½æ‰€æœ‰å¯ç”¨çš„ Agents å’Œ Groups åˆ°ä¸‹æ‹‰åˆ—è¡¨
     * @private
     */
    _loadAgents() {
        this.agentSelect.innerHTML = '<option value="">é€‰æ‹© Agent æˆ– Group...</option>';
        const executables = this.configManager.getAllLLMExecutables()
            .sort((a, b) => a.name.localeCompare(b.name));
        
        const groups = executables.filter(e => e.type === 'group');
        const agents = executables.filter(e => e.type === 'agent');

        if (groups.length > 0) {
            const groupOptgroup = document.createElement('optgroup');
            groupOptgroup.label = 'Agent Groups';
            groups.forEach(group => {
                const option = document.createElement('option');
                option.value = group.id;
                option.textContent = `${group.icon || 'ğŸ‘¥'} ${group.name}`;
                groupOptgroup.appendChild(option);
            });
            this.agentSelect.appendChild(groupOptgroup);
        }

        if (agents.length > 0) {
            const agentOptgroup = document.createElement('optgroup');
            agentOptgroup.label = 'Individual Agents';
            agents.forEach(agent => {
                const option = document.createElement('option');
                option.value = agent.id;
                option.textContent = `${agent.icon || 'ğŸ¤–'} ${agent.name}`;
                agentOptgroup.appendChild(option);
            });
            this.agentSelect.appendChild(agentOptgroup);
        }
    }

    /**
     * ç»‘å®š UI äº‹ä»¶ç›‘å¬å™¨
     * @private
     */
    _bindEvents() {
        this.sendButton.addEventListener('click', () => this._sendMessage());
        
        this.promptInput.addEventListener('keydown', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                this._sendMessage();
            }
        });
        
        this.promptInput.addEventListener('input', () => {
            const length = this.promptInput.value.length;
            this.charCount.textContent = length;
            this.charCount.style.color = length > 1000 ? '#ef4444' : '#9ca3af';
            
            this.promptInput.style.height = 'auto';
            this.promptInput.style.height = this.promptInput.scrollHeight + 'px';
        });
        
        this.agentSelect.addEventListener('change', (e) => {
            this.selectedAgentId = e.target.value;
        });
    }

    /**
     * âœ… æ­¥éª¤ 3: é›†ä¸­åŒ–UIæ›´æ–°
     * æ ¹æ®å½“å‰æ‰§è¡ŒçŠ¶æ€æ›´æ–°UIå…ƒç´ 
     * @private
     */
    _updateUIForExecutionState() {
        const isExecuting = this.executionState !== ExecutionState.IDLE;
        
        this.sendButton.disabled = isExecuting;
        this.promptInput.disabled = isExecuting;
        
        if (isExecuting) {
            this.sendButton.textContent = 'æ€è€ƒä¸­...';
            this.container.classList.add('is-executing');
        } else {
            this.sendButton.textContent = 'å‘é€';
            this.container.classList.remove('is-executing');
            this.promptInput.focus();
        }
    }

    /**
     * å‘é€ç”¨æˆ·æ¶ˆæ¯å¹¶æ‰§è¡Œ LLM è¯·æ±‚
     * 
     * @private
     * @async
     */
    async _sendMessage() {
        // âœ… ä½¿ç”¨çŠ¶æ€æœºåˆ¤æ–­ï¼Œé€»è¾‘æ›´æ¸…æ™°
        if (this.executionState !== ExecutionState.IDLE) return;

        const userInput = this.promptInput.value.trim();
        if (!userInput || !this.selectedAgentId) {
            if (!this.selectedAgentId) alert('è¯·å…ˆé€‰æ‹©ä¸€ä¸ª Agent æˆ– Group');
            return;
        }

        if (!this.currentTopic || !this.activeTopicId) {
            alert("æ— æ³•å‘é€æ¶ˆæ¯ï¼šæ²¡æœ‰æ´»åŠ¨çš„å¯¹è¯ã€‚");
            return;
        }

        // âœ… æ­¥éª¤ 4: ç®¡ç†çŠ¶æ€è½¬æ¢ - è¿›å…¥æ‰§è¡ŒçŠ¶æ€
        this.executionState = ExecutionState.EXECUTING_SINGLE; // é»˜è®¤è¿›å…¥å•ä¸€æ‰§è¡Œæ¨¡å¼
        this._updateUIForExecutionState();

        this._addMessageBlock('User', userInput);
        
        // --- THIS IS THE FIX ---
        // Step 2: DO NOT modify the history array here.
        // The Executor is responsible for constructing the new history.
        // REMOVED: this.currentTopic.history.push({ role: 'user', content: userInput });
        // --- END OF FIX ---

        this.promptInput.value = '';
        this.promptInput.style.height = 'auto';
        this.charCount.textContent = '0';

        this.parallelResponseBlocks.clear();
        this.currentResponseBlock = null;

        try {
            const executor = ExecutorFactory.create(this.selectedAgentId);

            // +++ æ¤å…¥è¯Šæ–­æ—¥å¿— (1/3) +++
            // éªŒè¯æˆ‘ä»¬åˆ›å»ºçš„å›è°ƒå¯¹è±¡æ˜¯å¦å®Œæ•´
            const callbacksForExecutor = this._buildCallbacks();
            console.log(
                '%c[ChatEditor._sendMessage] Creating and passing callbacks object:', 
                'color: blue; font-weight: bold;', 
                callbacksForExecutor
            );
            console.log(
                `%c[ChatEditor._sendMessage] Does onGroupStart exist before passing? -> ${typeof callbacksForExecutor.onGroupStart}`,
                'color: blue; font-weight: bold;'
            );
            // +++ ç»“æŸè¯Šæ–­æ—¥å¿— +++

            const result = await executor.run(
                userInput,
                this.currentTopic.history,
                {},
                callbacksForExecutor // ä½¿ç”¨æˆ‘ä»¬åˆšåˆšè®°å½•çš„å˜é‡
            );
            
            if (result.success && result.history) {
                this.currentTopic.history = result.history;
                
                if (this.currentTopic.title === 'New Topic' || this.currentTopic.title.startsWith('/New Topic')) {
                    const firstUserMsg = this.currentTopic.history.find(m => m.role === 'user');
                    if (firstUserMsg) {
                        this.currentTopic.title = this._cleanMarkdownForTitle(firstUserMsg.content).substring(0, 50);
                    }
                }

                // å…³é”®ï¼šä½¿ç”¨å†…éƒ¨çš„ provider å’Œ activeTopicId ä¿å­˜
                await this.topicProvider.saveItem(this.activeTopicId, { 
                    content: this.currentTopic 
                });
            }
            
        } catch (error) {
            console.error('Failed to execute:', error);
            this._addMessageBlock('System Error', `Failed to run: ${error.message}`);
        } finally {
            // âœ… æ­¥éª¤ 4: ç®¡ç†çŠ¶æ€è½¬æ¢ - è¿”å›ç©ºé—²çŠ¶æ€
            this.executionState = ExecutionState.IDLE;
            this._updateUIForExecutionState();
        }
    }

    /**
     * âœ… æ­¥éª¤ 5: ç®€åŒ–å›è°ƒé€»è¾‘
     * æ„å»ºå›è°ƒå¯¹è±¡ï¼Œç°åœ¨ä½¿ç”¨çŠ¶æ€æœºæ¥å†³å®šé€»è¾‘åˆ†æ”¯
     * 
     * @returns {Object} å›è°ƒå¯¹è±¡
     * @private
     */
    _buildCallbacks() {
        return {
            onStepStart: (step, data, isAuto, agentId) => {
                if (isAuto) this._addSystemMessageBlock(data.title, data.prompt);
            },
            
            onError: (error, agentId) => {
            console.error('[ChatEditor] onError:', error, 'agentId:', agentId);
                const executable = agentId ? this.configManager.getLLMExecutable(agentId) : null;
                const displayName = executable ? `${executable.icon || 'ğŸ¤–'} ${executable.name}` : 'Agent';
                
                // ä½¿ç”¨çŠ¶æ€æœºæ¥å¤„ç†é”™è¯¯æ˜¾ç¤º
                if (this.executionState === ExecutionState.EXECUTING_PARALLEL) {
                    const blockInfo = this.parallelResponseBlocks.get(agentId);
                    if (blockInfo) {
                        blockInfo.editor.setText(`\n\n**Error:** ${error.message}`);
                        blockInfo.editor.setTitle(`${displayName}: Error`);
                    } else {
                        // If agentId is not found in parallel blocks, it might be a Group error or something else
                        console.warn(`[ChatEditor] Error occurred, but no specific parallel block found for agentId: ${agentId}`);
                        // Optionally, you could create a generic error block or append to a main error log
                        this._addMessageBlock(`${displayName} Error`, error.message);
                    }
                } else if (this.currentResponseBlock) {
                    this.currentResponseBlock.setText(`\n\n**Error:** ${error.message}`);
                    this.currentResponseBlock.setTitle(`${displayName}: Error`);
                } else {
                    this._addMessageBlock(`${displayName} Error`, error.message);
                }
            },

            /**
             * Group æ‰§è¡Œå¼€å§‹å›è°ƒ
             * @param {Object} data - Group æ•°æ®
             * @param {string} data.mode - æ‰§è¡Œæ¨¡å¼
             * @param {Array<string>} data.agents - Agent ID åˆ—è¡¨
             */
            onGroupStart: (data) => {
                // è¿™ä¸ªæ—¥å¿—ç°åœ¨è‡³å…³é‡è¦
                console.log('%c[ChatEditor.onGroupStart] CALLBACK TRIGGERED!', 'color: green; font-weight: bold;', data);
                if (data?.mode === 'parallel') {
                    console.log('[ChatEditor] Switching to PARALLEL execution state.');
                    this.executionState = ExecutionState.EXECUTING_PARALLEL;
                    
                    this.parallelResponseBlocks.clear();
                    // --- FIX ---
                    // Ensure we are creating blocks for the direct children agents, not the group itself
                    if (data.agents && Array.isArray(data.agents)) {
                         data.agents.forEach(agentId => { // agentId here is the child agentId
                            const executable = this.configManager.getLLMExecutable(agentId);
                            const displayName = executable ? `${executable.icon || 'ğŸ¤–'} ${executable.name}` : agentId;
                            this._createParallelResponseBlock(agentId, displayName); // Use child agentId as key
                        });
                    } else {
                         console.warn('[ChatEditor] onGroupStart received for parallel mode, but data.agents is missing or not an array.');
                    }
                    // --- END FIX ---
                }
            },

            onResponseStart: (step, agentId, displayName) => { // agentId here is crucial
                console.log(`[ChatEditor] onResponseStart: State=${this.executionState}, AgentId=${agentId}`);
                switch (this.executionState) {
                    case ExecutionState.EXECUTING_PARALLEL: {
                        const blockInfo = this.parallelResponseBlocks.get(agentId);
                        if (blockInfo) {
                            blockInfo.editor.setTitle(`${displayName || 'Agent'} is typing...`);
                        } else {
                            console.warn(`[ChatEditor] No parallel block found for agentId: ${agentId}`);
                        }
                        break;
                    }
                    case ExecutionState.EXECUTING_SINGLE: {
                        const finalDisplayName = displayName || 'Assistant';
                        this.currentResponseBlock = this._addMessageBlock(finalDisplayName, '');
                        this.currentResponseBlock.setTitle(`${finalDisplayName} is typing...`);
                        this._isFirstChunk = true;
                        break;
                    }
                }
            },
            
            onChunk: (chunk, agentId) => {
                // âœ… ä½¿ç”¨çŠ¶æ€æœºï¼Œé€»è¾‘åˆ†æ”¯æ¸…æ™°
                switch (this.executionState) {
                    case ExecutionState.EXECUTING_PARALLEL: {
                        const blockInfo = this.parallelResponseBlocks.get(agentId);
                        if (blockInfo) {
                            blockInfo.editor.appendStreamChunk(null, chunk, blockInfo.isFirstChunk);
                            // æ¯ä¸ªå—ç‹¬ç«‹ç®¡ç†è‡ªå·±çš„ isFirstChunk çŠ¶æ€
                            if (blockInfo.isFirstChunk) {
                                blockInfo.isFirstChunk = false;
                            }
                        } else {
                            console.warn(`[ChatEditor] Received chunk, but no parallel block found for agentId: ${agentId}`);
                        }
                        break;
                    }
                    case ExecutionState.EXECUTING_SINGLE: {
                        if (this.currentResponseBlock) {
                            this.currentResponseBlock.appendStreamChunk(null, chunk, this._isFirstChunk);
                            if (this._isFirstChunk) this._isFirstChunk = false;
                        }
                        break;
                    }
                }
            },
            
            onStepEnd: (step, response, agentId) => {
                const executable = agentId ? this.configManager.getLLMExecutable(agentId) : null;
                const displayName = executable ? `${executable.icon || 'ğŸ¤–'} ${executable.name}` : 'Assistant';
                const cleanResponse = this._cleanMarkdownForTitle(response);
                // FIX: ç¡®ä¿æ ‡é¢˜èƒ½æ­£ç¡®æ˜¾ç¤º Agent åç§°
                const title = `${displayName}: ${cleanResponse.substring(0, 40)}${cleanResponse.length > 40 ? '...' : ''}`;

                // âœ… ä½¿ç”¨çŠ¶æ€æœºï¼Œé€»è¾‘åˆ†æ”¯æ¸…æ™°
                switch (this.executionState) {
                    case ExecutionState.EXECUTING_PARALLEL: {
                        const blockInfo = this.parallelResponseBlocks.get(agentId);
                        if (blockInfo) {
                             blockInfo.editor.setTitle(title);
                        } else {
                             console.warn(`[ChatEditor] Step ended, but no parallel block found for agentId: ${agentId}`);
                        }
                        // --- END FIX ---
                        break;
                    }
                    case ExecutionState.EXECUTING_SINGLE: {
                        if (this.currentResponseBlock) {
                            this.currentResponseBlock.setTitle(title);
                            this.currentResponseBlock = null;
                        }
                        break;
                    }
                }
            },
            
            onGroupEnd: (result) => {
                console.log('[ChatEditor] onGroupEnd received:', result);
                // --- FIX ---
                // We need to differentiate between serial and parallel group ends
                if (result?.mode === 'parallel' && result?.type === 'group_response') {
                    // This is the end of a parallel group execution
                    console.log('[ChatEditor] Handling end of PARALLEL group.');
                    
                    // Clean up the temporary parallel response blocks
                    this.parallelResponseBlocks.forEach((blockInfo) => {
                        const blockElement = document.getElementById(blockInfo.blockId);
                        if (blockElement) blockElement.remove();
                        this.messageBlocks.delete(blockInfo.blockId);
                    });
                    this.parallelResponseBlocks.clear();
                    
                    // æ¸²æŸ“æœ€ç»ˆçš„èšåˆç»“æœ
                    this._renderGroupResponse(result);

                    // --- å…³é”®ä¿®å¤ï¼šé‡ç½®çŠ¶æ€ï¼ ---
                    // å¹¶è¡Œéƒ¨åˆ†å·²ç»å®Œå…¨ç»“æŸï¼Œåç»­çš„æ­¥éª¤ï¼ˆå¦‚æœæœ‰ï¼‰åº”è¯¥æ˜¯ä¸²è¡Œçš„ã€‚
                    // æˆ‘ä»¬å°†çŠ¶æ€åˆ‡æ¢å› SINGLEï¼Œä»¥ä¾¿æ­£ç¡®å¤„ç†åç»­çš„ Agentã€‚
                    console.log('[ChatEditor] Parallel part finished. Switching back to SINGLE execution state.');
                    this.executionState = ExecutionState.EXECUTING_SINGLE;
                    // --- ä¿®å¤ç»“æŸ ---

                } else if (result?.finalResponse !== undefined) { 
                    // This might be the end of a serial group or a single agent final response
                    // If it's a serial group, the final response is already handled by the last agent's onStepEnd
                    // We might need to add logic here if serial groups also produce a distinct aggregated view
                    console.log('[ChatEditor] Handling end of SERIAL group or final response.');
                    // For now, assume serial group results are implicitly handled by chaining.
                    // If a serial group needs special aggregation UI, add logic here.
                } else {
                    console.warn('[ChatEditor] Received unknown group end result:', result);
                }
                // --- END FIX ---
            }
        };
    }

    _selectDefaultAgent(topic, context) {
        let agentIdToSelect = null;
        const lastMessageWithAgent = [...(topic?.history || [])].reverse().find(msg => msg.agentId);
        if (lastMessageWithAgent?.agentId) {
            agentIdToSelect = lastMessageWithAgent.agentId;
        } else if (context.preferredAgentId && context.preferredAgentId !== 'all') {
            agentIdToSelect = context.preferredAgentId;
        } else {
            agentIdToSelect = this.agentSelect.options[1]?.value;
        }
        if (agentIdToSelect) {
            this.agentSelect.value = agentIdToSelect;
            this.selectedAgentId = agentIdToSelect;
        }
    }

    /**
     * æ¸…ç† Markdown æ ‡è®°ï¼ˆç”¨äºç”Ÿæˆç®€æ´çš„æ ‡é¢˜ï¼‰
     * 
     * @param {string} text - åŸå§‹æ–‡æœ¬
     * @returns {string} æ¸…ç†åçš„æ–‡æœ¬
     * @private
     */
    _cleanMarkdownForTitle(text) {
        return text
            .replace(/\*\*(.+?)\*\*/g, '$1')
            .replace(/\*(.+?)\*/g, '$1')
            .replace(/__(.+?)__/g, '$1')
            .replace(/_(.+?)_/g, '$1')
            .replace(/~~(.+?)~~/g, '$1')
            .replace(/`(.+?)`/g, '$1')
            .replace(/\[(.+?)\]\(.+?\)/g, '$1')
            .replace(/^#+\s+/gm, '')
            .replace(/\n/g, ' ')
            .trim();
    }

    /**
     * è®¾ç½®ç¼–è¾‘å™¨çš„æ–‡æœ¬å†…å®¹
     * @param {string} markdown - Markdown æ ¼å¼çš„æ–‡æœ¬
     */
    setText(markdown) {
        this.mainArea.innerHTML = '';
        this.messageBlocks.clear();
        if (markdown) {
            this._addMessageBlock('', markdown);
        }
    }

    /**
     * è·å–ç¼–è¾‘å™¨çš„æ–‡æœ¬å†…å®¹
     * @returns {string} æ‰€æœ‰æ¶ˆæ¯å—çš„åˆå¹¶å†…å®¹
     */
    getText() {
        return Array.from(this.messageBlocks.values())
            .map(editor => editor.getText())
            .join('\n\n---\n\n');
    }

    /**
     * è®¾ç½®ç¼–è¾‘å™¨æ ‡é¢˜ï¼ˆChatEditor ä¸ä½¿ç”¨æ­¤æ–¹æ³•ï¼‰
     * @param {string} newTitle - æ–°æ ‡é¢˜
     */
    setTitle(newTitle) { 
    }
    
    /**
     * è®¾ç½®åªè¯»çŠ¶æ€ï¼ˆChatEditor çš„åªè¯»çŠ¶æ€ç”±å†…éƒ¨æ§åˆ¶ï¼‰
     * @param {boolean} isReadOnly - æ˜¯å¦åªè¯»
     */
    setReadOnly(isReadOnly) { 
    }
    
    // +++ NEW PUBLIC API METHOD: To set input state while respecting encapsulation +++
    /**
     * Sets the initial state of the input bar from an external component.
     * @param {object} state - The state to set.
     * @param {string} [state.prompt] - The text to put in the prompt input box.
     * @param {string} [state.agentId] - The ID of the agent to select.
     * @param {boolean} [state.submit=false] - If true, immediately triggers the send message action.
     */
    setInputState({ prompt, agentId, submit = false }) {
        if (prompt !== undefined && this.promptInput) {
            this.promptInput.value = prompt;
            // Manually trigger the 'input' event to update UI (char count, height)
            this.promptInput.dispatchEvent(new Event('input', { bubbles: true }));
        }

        if (agentId !== undefined && this.agentSelect) {
            // Check if the option exists before setting the value
            if (this.agentSelect.querySelector(`option[value="${agentId}"]`)) {
                this.agentSelect.value = agentId;
                // Manually trigger 'change' event to update internal state (this.selectedAgentId)
                this.agentSelect.dispatchEvent(new Event('change', { bubbles: true }));
            } else {
                console.warn(`[ChatEditor] Attempted to select non-existent agentId: ${agentId}`);
            }
        }
        
        // --- NEW LOGIC ---
        // If the submit flag is true, trigger the send message action.
        // We use a microtask (Promise.resolve) to ensure the DOM and state have updated before sending.
        if (submit) {
           Promise.resolve().then(() => {
               this._sendMessage();
           });
        }
    }
    // +++ END NEW METHOD +++

    /**
     * èšç„¦åˆ°è¾“å…¥æ¡†
     */
    focus() { 
        this.promptInput.focus(); 
    }

    /**
     * é”€æ¯ç¼–è¾‘å™¨å¹¶æ¸…ç†èµ„æº
     */
    destroy() {
        this.providerUnsubscribers.forEach(unsub => unsub());
        this.providerUnsubscribers = [];
        this.messageBlocks.forEach(editor => editor.destroy());
        this._listeners.clear();
        this.container.innerHTML = '';
    }

    /**
     * æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
     * 
     * @param {string} eventName - äº‹ä»¶åç§°ï¼ˆå¦‚ 'toggle-sidebar-request'ï¼‰
     * @param {Function} callback - å›è°ƒå‡½æ•°
     * @returns {Function} å–æ¶ˆè®¢é˜…å‡½æ•°
     * 
     * @example
     * const unsubscribe = editor.on('toggle-sidebar-request', () => {
     *   console.log('Sidebar toggle requested');
     * });
     * unsubscribe();
     */
    on(eventName, callback) {
        if (!this._listeners.has(eventName)) {
            this._listeners.set(eventName, []);
        }
        this._listeners.get(eventName).push(callback);
        return () => this.off(eventName, callback);
    }

    /**
     * ç§»é™¤äº‹ä»¶ç›‘å¬å™¨
     * 
     * @param {string} eventName - äº‹ä»¶åç§°
     * @param {Function} callback - è¦ç§»é™¤çš„å›è°ƒå‡½æ•°
     */
    off(eventName, callback) {
        if (!this._listeners.has(eventName)) return;
        const listeners = this._listeners.get(eventName);
        const index = listeners.indexOf(callback);
        if (index > -1) listeners.splice(index, 1);
    }

    /**
     * è§¦å‘äº‹ä»¶
     * 
     * @param {string} eventName - äº‹ä»¶åç§°
     * @param {*} data - äº‹ä»¶æ•°æ®
     * @private
     */
    _emit(eventName, data) {
        if (!this._listeners.has(eventName)) return;
        this._listeners.get(eventName).forEach(callback => {
            try {
                callback(data);
            } catch (error) {
                console.error(`Error in ChatEditor event listener for ${eventName}:`, error);
            }
        });
    }
    
    /**
     * è·å–å¯æœç´¢çš„çº¯æ–‡æœ¬å†…å®¹
     * @returns {Promise<string>} çº¯æ–‡æœ¬å†…å®¹
     */
    async getSearchableText() { 
        return this.getText().replace(/^#+\s/gm, '').replace(/\[(.*?)\]\(.*?\)/g, '$1').trim(); 
    }
    
    /**
     * è·å–æ–‡æ¡£æ ‡é¢˜åˆ—è¡¨
     * @returns {Promise<Array>} æ ‡é¢˜æ•°ç»„
     */
    async getHeadings() { 
        return []; 
    }
    
    /**
     * è·å–æ–‡æ¡£æ‘˜è¦
     * @returns {Promise<string>} æ–‡æ¡£æ‘˜è¦
     */
    async getSummary() {
        const text = this.getText();
        return text.split('\n').slice(0, 5).join('\n');
    }
    
    /**
     * å¯¼èˆªåˆ°æŒ‡å®šä½ç½®
     * @param {*} target - å¯¼èˆªç›®æ ‡
     * @param {Object} options - å¯¼èˆªé€‰é¡¹
     * @returns {Promise<void>}
     */
    async navigateTo(target, options) { 
    }
    
    /**
     * æœç´¢å†…å®¹
     * @param {string} query - æœç´¢æŸ¥è¯¢
     * @returns {Promise<Array>} æœç´¢ç»“æœ
     */
    async search(query) { 
        return []; 
    }
    
    /**
     * è·³è½¬åˆ°æœç´¢ç»“æœ
     * @param {*} result - æœç´¢ç»“æœ
     */
    gotoMatch(result) { }
    
    /**
     * æ¸…é™¤æœç´¢é«˜äº®
     */
    clearSearch() { }
}
/**
 * @file src/llm/executor/AgentGroupExecutor.js
 * @description è´Ÿè´£ç¼–æ’å’Œæ‰§è¡Œ Agent Group çš„é€»è¾‘ï¼ˆå¹¶è¡Œæˆ–ä¸²è¡Œæ¨¡å¼ï¼‰
 */

import { IExecutor } from './IExecutor.js';
import { ExecutorFactory } from './ExecutorFactory.js';
import { ConfigManager } from '../../config/configManager.js';

/**
 * @typedef {Object} ParallelResponse
 * @property {string} agentId - Agent ID
 * @property {string} content - å“åº”å†…å®¹
 * @property {boolean} success - æ˜¯å¦æˆåŠŸ
 */

/**
 * @typedef {Object} GroupResponseStructure
 * @property {'group_response'} type - å“åº”ç±»å‹æ ‡è¯†
 * @property {'parallel'|'serial'} mode - æ‰§è¡Œæ¨¡å¼
 * @property {Array<ParallelResponse>} responses - æ‰€æœ‰å“åº”çš„æ•°ç»„
 */

/**
 * @class AgentGroupExecutor
 * @extends IExecutor
 * @description æ‰§è¡Œ Agent Group çš„é€»è¾‘ï¼Œæ”¯æŒï¼š
 * - å¹¶è¡Œæ¨¡å¼ï¼ˆparallelï¼‰ï¼šæ‰€æœ‰å­ Agent åŒæ—¶æ‰§è¡Œ
 * - ä¸²è¡Œæ¨¡å¼ï¼ˆserialï¼‰ï¼šå­ Agent æŒ‰é¡ºåºæ‰§è¡Œï¼Œå‰ä¸€ä¸ªçš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªçš„è¾“å…¥
 */
export class AgentGroupExecutor extends IExecutor {
    /**
     * @param {string} groupId - Group çš„ ID
     * @throws {Error} å¦‚æœ Group ä¸å­˜åœ¨æˆ–ç±»å‹ä¸æ­£ç¡®
     */
    constructor(groupId) {
        super();
        this.groupId = groupId;
        this.configManager = ConfigManager.instance();
        this.group = this.configManager.getLLMExecutable(this.groupId);
        
        if (!this.group || this.group.type !== 'group') {
            throw new Error(`GroupExecutable with ID '${groupId}' not found or is not a group type.`);
        }
    }

    /**
     * æ‰§è¡Œ Groupï¼ˆæ ¹æ®æ¨¡å¼è‡ªåŠ¨é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œï¼‰
     * 
     * @param {string} userPrompt - ç”¨æˆ·çš„åˆå§‹è¾“å…¥
     * @param {Array<import('./IExecutor.js').MessageHistoryItem>} [externalHistory=[]] - å¤–éƒ¨ä¼ å…¥çš„å¯¹è¯å†å²
     * @param {Object} [initialVariables={}] - åˆå§‹å˜é‡ä¸Šä¸‹æ–‡
     * @param {import('./IExecutor.js').ExecutorCallbacks} [callbacks={}] - ç”¨äºæµå¼å“åº”å’Œè¿›åº¦çš„å›è°ƒ
     * @returns {Promise<import('./IExecutor.js').ExecutorResult>}
     */
    async run(userPrompt, externalHistory = [], initialVariables = {}, callbacks = {}) {
        // +++ æ¤å…¥è¯Šæ–­æ—¥å¿— (2/3) +++
        // éªŒè¯æˆ‘ä»¬æ¥æ”¶åˆ°çš„å›è°ƒå¯¹è±¡æ˜¯å¦å®Œæ•´
        console.log(
            '%c[AgentGroupExecutor.run] Received callbacks object:', 
            'color: orange; font-weight: bold;', 
            callbacks
        );
        console.log(
            `%c[AgentGroupExecutor.run] Does onGroupStart exist upon reception? -> ${typeof callbacks.onGroupStart}`,
            'color: orange; font-weight: bold;'
        );
        // +++ ç»“æŸè¯Šæ–­æ—¥å¿— +++

        if (this.group.mode === 'parallel') {
            return this._runParallel(userPrompt, externalHistory, initialVariables, callbacks);
        } else {
            return this._runSerial(userPrompt, externalHistory, initialVariables, callbacks);
        }
    }

    /**
     * å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼šæ‰€æœ‰å­ Agent åŒæ—¶æ‰§è¡Œ
     * 
     * @private
     * @param {string} userPrompt - ç”¨æˆ·è¾“å…¥
     * @param {Array<import('./IExecutor.js').MessageHistoryItem>} externalHistory - å¯¹è¯å†å²
     * @param {Object} initialVariables - åˆå§‹å˜é‡
     * @param {import('./IExecutor.js').ExecutorCallbacks} callbacks - å›è°ƒå‡½æ•°
     * @returns {Promise<import('./IExecutor.js').ExecutorResult>}
     */
    async _runParallel(userPrompt, externalHistory, initialVariables, callbacks) {
        console.log(`[AgentGroupExecutor] Starting parallel execution`);
        console.log(`[AgentGroupExecutor] Group children:`, this.group.children);
        
        // +++ æ¤å…¥è¯Šæ–­æ—¥å¿— (3/3) +++
        // åœ¨è°ƒç”¨å‰æœ€åä¸€åˆ»ï¼Œå†æ¬¡æ£€æŸ¥å›è°ƒå¯¹è±¡
        console.log(
            '%c[AgentGroupExecutor._runParallel] About to call onGroupStart. Callbacks object is:', 
            'color: red; font-weight: bold;', 
            callbacks
        );
        console.log(
            `%c[AgentGroupExecutor._runParallel] Does onGroupStart exist right before call? -> ${typeof callbacks.onGroupStart}`,
            'color: red; font-weight: bold;'
        );
        // +++ ç»“æŸè¯Šæ–­æ—¥å¿— +++

        try {
            callbacks.onGroupStart?.({
                mode: 'parallel',
                agents: this.group.children || [],
                groupId: this.group.id
            });
        } catch (err) {
            console.error('[AgentGroupExecutor] FATAL: Error calling onGroupStart:', err);
        }

        const historyWithUserPrompt = [...externalHistory, { role: 'user', content: userPrompt }];
        
        const promises = this.group.children.map(childId => {
            const subExecutor = ExecutorFactory.create(childId);
            const executable = this.configManager.getLLMExecutable(childId);
            const displayName = executable ? `${executable.icon || 'ğŸ¤–'} ${executable.name}` : childId;
            
    console.log(`[Parallel] Creating executor for agent: ${childId}, displayName: ${displayName}`);
            return subExecutor.run(
                userPrompt,
                externalHistory, 
                initialVariables,
                {
                    ...callbacks, // ç»§æ‰¿æ‰€æœ‰å›è°ƒä»¥é˜²ä¸‡ä¸€
                    // æ£€æŸ¥ä¸‹ä¸€å±‚æ˜¯å¦å·²æä¾›äº† agentIdã€‚å¦‚æœæ²¡æœ‰ï¼Œæˆ‘ä»¬æ‰æä¾›è‡ªå·±çš„ childIdã€‚
                    // è¿™æ ·æœ€åº•å±‚çš„ Agent ID (e.g., 'agent-dreamer') å°±èƒ½ä¸€è·¯å†’æ³¡åˆ°æœ€é¡¶å±‚ã€‚
                    onStepStart: (step, data, isAuto, agentId) => callbacks.onStepStart?.(step, data, isAuto, agentId || childId),
                    onResponseStart: (step, agentId, dn) => callbacks.onResponseStart?.(step, agentId || childId, dn || displayName),
                    onChunk: (chunk, agentId) => callbacks.onChunk?.(chunk, agentId || childId),
                    onStepEnd: (step, response, agentId) => callbacks.onStepEnd?.(step, response, agentId || childId),
                    onError: (error, agentId) => callbacks.onError?.(error, agentId || childId),
                }
            );
        });

        const results = await Promise.allSettled(promises);

        const parallelResponses = results.map((result, index) => {
            if (result.status === 'fulfilled') {
                return {
                    agentId: this.group.children[index],
                    content: result.value.finalResponse,
                    success: result.value.success
                };
            } else {
                console.error(`Agent ${this.group.children[index]} failed:`, result.reason);
                return {
                    agentId: this.group.children[index],
                    content: `Execution failed: ${result.reason.message}`,
                    success: false
                };
            }
        });

        /** @type {GroupResponseStructure} */
        const structuredResponseContent = {
            type: 'group_response',
            mode: 'parallel',
            responses: parallelResponses.filter(r => r.success)
        };

        const summaryForLLM = "Multiple agents responded in parallel:\n\n" +
            parallelResponses
                .filter(r => r.success)
                .map(r => {
                    const agent = this.configManager.getLLMExecutable(r.agentId);
                    const agentName = agent?.name || r.agentId;
                    return `--- Response from ${agentName} ---\n${r.content}`;
                }).join('\n\n');
        
        const finalHistory = [
            ...historyWithUserPrompt,
            {
                role: 'assistant',
                content: summaryForLLM,  
                _structuredContent: structuredResponseContent,  
                agentId: this.groupId 
            }
        ];

        callbacks.onGroupEnd?.(structuredResponseContent);
        
        return { 
            success: true, 
            history: finalHistory, 
            finalResponse: summaryForLLM  
        };
    }

    /**
     * ä¸²è¡Œæ‰§è¡Œæ¨¡å¼ï¼šå­ Agent æŒ‰é¡ºåºæ‰§è¡Œï¼Œå‰ä¸€ä¸ªçš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªçš„è¾“å…¥
     * 
     * @private
     * @param {string} userPrompt - ç”¨æˆ·è¾“å…¥
     * @param {Array<import('./IExecutor.js').MessageHistoryItem>} externalHistory - å¯¹è¯å†å²
     * @param {Object} initialVariables - åˆå§‹å˜é‡
     * @param {import('./IExecutor.js').ExecutorCallbacks} callbacks - å›è°ƒå‡½æ•°
     * @returns {Promise<import('./IExecutor.js').ExecutorResult>}
     */
    async _runSerial(userPrompt, externalHistory, initialVariables, callbacks) {
        callbacks.onGroupStart?.({ mode: 'serial', agents: this.group.children });

        let currentPrompt = userPrompt;
        let cumulativeHistory = JSON.parse(JSON.stringify(externalHistory)); 
        let lastResponse = '';
        let context = { ...initialVariables, user_prompt: userPrompt };

        for (const childId of this.group.children) {
            const subExecutor = ExecutorFactory.create(childId);
            const executable = this.configManager.getLLMExecutable(childId);
            const displayName = executable ? `${executable.icon || 'ğŸ¤–'} ${executable.name}` : childId;
            
            callbacks.onAgentStart?.(childId, displayName);

            const result = await subExecutor.run(
                currentPrompt,  
                cumulativeHistory,
                context,
                // --- æ ¸å¿ƒä¿®å¤ ---
                // ä½¿ç”¨æ‰©å±•è¿ç®—ç¬¦ `...` æ¥ç»§æ‰¿æ‰€æœ‰åŸå§‹çš„å›è°ƒå‡½æ•°ï¼Œ
                // ç„¶ååªè¦†ç›–æˆ‘ä»¬éœ€è¦ä¿®æ”¹è¡Œä¸ºçš„é‚£äº›ã€‚
                // è¿™ç¡®ä¿äº†åƒ onGroupStart, onGroupEnd è¿™æ ·çš„å›è°ƒèƒ½è¢«å®Œæ•´åœ°ä¼ é€’ä¸‹å»ã€‚
                {
                    ...callbacks,
                    onStepStart: (step, data, isAuto, agentId) => callbacks.onStepStart?.(step, data, isAuto, agentId || childId),
                    onResponseStart: (step, agentId, dn) => callbacks.onResponseStart?.(step, agentId || childId, dn || displayName),
                    onChunk: (chunk, agentId) => callbacks.onChunk?.(chunk, agentId || childId),
                    onStepEnd: (step, response, agentId) => callbacks.onStepEnd?.(step, response, agentId || childId),
                    onError: (error, agentId) => callbacks.onError?.(error, agentId || childId),
                }
            );

            if (!result.success) {
                const error = new Error(`Serial execution failed at: ${displayName}`);
                callbacks.onError?.(error, childId);
                return { 
                    success: false, 
                    history: cumulativeHistory, 
                    finalResponse: lastResponse 
                };
            }

            cumulativeHistory = result.history;
            currentPrompt = result.finalResponse;  
            lastResponse = result.finalResponse;
            context['last_response'] = lastResponse;
        }

        callbacks.onGroupEnd?.({ finalResponse: lastResponse });
        
        return { 
            success: true, 
            history: cumulativeHistory, 
            finalResponse: lastResponse  
        };
    }
}/**
 * @file src/llm/executor/IExecutor.js
 * @description å®šä¹‰æ‰€æœ‰å¯æ‰§è¡Œå•å…ƒï¼ˆAgent, Groupç­‰ï¼‰å¿…é¡»å®ç°çš„ç»Ÿä¸€æ¥å£ã€‚
 * 
 * æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
 * 1. finalResponse å¿…é¡»å§‹ç»ˆæ˜¯å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿å¯ä»¥ä¸²è”æ‰§è¡Œ
 * 2. ç»“æ„åŒ–æ•°æ®åº”è¯¥å­˜å‚¨åœ¨ history çš„ _structuredContent å­—æ®µä¸­
 * 3. æ‰€æœ‰ Executor éƒ½åº”è¯¥èƒ½å¤Ÿæ— ç¼ç»„åˆå’ŒåµŒå¥—
 */

/**
 * @typedef {Object} ExecutorResult
 * @property {boolean} success - æ‰§è¡Œæ˜¯å¦æˆåŠŸ
 * @property {Array<MessageHistoryItem>} history - å®Œæ•´çš„å¯¹è¯å†å²è®°å½•
 * @property {string} finalResponse - æœ€ç»ˆå“åº”çš„æ–‡æœ¬å†…å®¹ï¼ˆå¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œç”¨äºä¸²è”ï¼‰
 * @property {Object} [metadata] - å¯é€‰çš„æ‰§è¡Œå…ƒæ•°æ®
 * @property {number} [metadata.executionTime] - æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
 * @property {number} [metadata.tokensUsed] - æ¶ˆè€—çš„ token æ•°é‡
 */

/**
 * @typedef {Object} MessageHistoryItem
 * @property {'system'|'user'|'assistant'} role - æ¶ˆæ¯è§’è‰²
 * @property {string} content - æ¶ˆæ¯å†…å®¹ï¼ˆå¿…é¡»æ˜¯å­—ç¬¦ä¸²æ–‡æœ¬ï¼‰
 * @property {string} [agentId] - ç”Ÿæˆæ­¤æ¶ˆæ¯çš„ Agent ID
 * @property {Object} [_structuredContent] - å†…éƒ¨ç»“æ„åŒ–æ•°æ®ï¼ˆä¸ä¼šå‘é€ç»™LLMï¼‰
 */

/**
 * @typedef {Object} ExecutorCallbacks
 * @property {(step: number, data: {title: string, prompt: string}, isAuto: boolean, agentId?: string) => void} [onStepStart] - æ­¥éª¤å¼€å§‹å›è°ƒ
 * @property {(step: number, agentId?: string, displayName?: string) => void} [onResponseStart] - å“åº”å¼€å§‹å›è°ƒ
 * @property {(chunk: string, agentId?: string) => void} [onChunk] - æµå¼å“åº”ç‰‡æ®µå›è°ƒ
 * @property {(step: number, response: string, agentId?: string) => void} [onStepEnd] - æ­¥éª¤ç»“æŸå›è°ƒ
 * @property {(error: Error, agentId?: string) => void} [onError] - é”™è¯¯å›è°ƒ
 * @property {(data: {mode: string, agents: string[]}) => void} [onGroupStart] - Group å¼€å§‹å›è°ƒ
 * @property {(result: Object) => void} [onGroupEnd] - Group ç»“æŸå›è°ƒ
 * @property {(agentId: string, displayName: string) => void} [onAgentStart] - Agent å¼€å§‹å›è°ƒï¼ˆç”¨äº serialï¼‰
 * @property {(agentId: string, displayName: string) => void} [onAgentResponseStart] - Agent å“åº”å¼€å§‹ï¼ˆç”¨äº parallelï¼‰
 * @property {(agentId: string, chunk: string) => void} [onAgentChunk] - Agent å“åº”ç‰‡æ®µï¼ˆç”¨äº parallelï¼‰
 * @property {(agentId: string, response: string) => void} [onAgentStepEnd] - Agent æ­¥éª¤ç»“æŸï¼ˆç”¨äº parallelï¼‰
 * @property {(agentId: string, error: Error) => void} [onAgentError] - Agent é”™è¯¯ï¼ˆç”¨äº parallelï¼‰
 */

/**
 * @class IExecutor
 * @description æ‰€æœ‰æ‰§è¡Œå™¨çš„æŠ½è±¡åŸºç±»
 */
export class IExecutor {
    /**
     * æ‰§è¡Œå•å…ƒçš„æ ¸å¿ƒæ–¹æ³•ã€‚
     * 
     * @param {string} userPrompt - ç”¨æˆ·çš„åˆå§‹è¾“å…¥ï¼ˆå¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼‰
     * @param {Array<MessageHistoryItem>} [externalHistory=[]] - å¤–éƒ¨ä¼ å…¥çš„å¯¹è¯å†å²
     * @param {Object} [initialVariables={}] - åˆå§‹å˜é‡ä¸Šä¸‹æ–‡ï¼Œç”¨äºæ¨¡æ¿æ›¿æ¢
     * @param {ExecutorCallbacks} [callbacks={}] - ç”¨äºæµå¼å“åº”å’Œè¿›åº¦çš„å›è°ƒå¯¹è±¡
     * @returns {Promise<ExecutorResult>}
     * @throws {Error} å¦‚æœå­ç±»æœªå®ç°æ­¤æ–¹æ³•
     */
    async run(userPrompt, externalHistory = [], initialVariables = {}, callbacks = {}) {
        throw new Error("IExecutor subclass must implement the 'run' method.");
    }
}// æ–‡ä»¶: src/llm/core/LLMService.js

// [ä¿®æ­£] å¯¼å…¥ ConfigManager ç±»ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå·¥å‚å‡½æ•°
import { ConfigManager } from '../../config/configManager.js';
import { LLMClient } from './client.js';
// [ä¿®æ­£] å¯¼å…¥æˆ‘ä»¬ä¹‹å‰å®šä¹‰å¥½çš„ EVENTS
import { EVENTS } from '../../config/events.js'; // å‡è®¾ EVENTS åœ¨è¿™ä¸ªæ–‡ä»¶é‡Œ

// --- å•ä¾‹æ§åˆ¶ ---
let instance = null;

/**
 * @class LLMService
 * @singleton
 * @description
 * [V3 - æœåŠ¡å®¹å™¨æ¶æ„] ä½œä¸ºé…ç½®å±‚ (ConfigManager) å’Œæ ¸å¿ƒ LLM é€»è¾‘å±‚ (LLMClient) ä¹‹é—´çš„æ¡¥æ¢ã€‚
 * å®ƒçš„èŒè´£æ˜¯ï¼š
 * 1. ä½œä¸ºä¸€ä¸ªå…¨å±€å•ä¾‹æœåŠ¡å­˜åœ¨ã€‚
 * 2. ä¾èµ– `ConfigManager` å•ä¾‹æ¥è·å– LLM è¿æ¥é…ç½®ã€‚
 * 3. åˆ›å»ºå’Œç¼“å­˜ `LLMClient` å®ä¾‹ï¼Œé¿å…é‡å¤å®ä¾‹åŒ–ã€‚
 * 4. å“åº”å¼åœ°ç›‘å¬æ¥è‡ª `ConfigManager` çš„é…ç½®å˜æ›´äº‹ä»¶ï¼Œå¹¶è‡ªåŠ¨æ¸…ç†è¿‡æ—¶çš„å®¢æˆ·ç«¯ç¼“å­˜ï¼Œ
 *    ç¡®ä¿ç³»ç»Ÿå§‹ç»ˆä½¿ç”¨æœ€æ–°çš„è¿æ¥ä¿¡æ¯ã€‚
 */
export class LLMService {
    constructor() {
        if (instance) {
            // [ä¿®æ­£] è¿”å›å•ä¾‹å®ä¾‹è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ï¼Œæ›´ç¬¦åˆå•ä¾‹æ¨¡å¼
            return instance;
        }
        
        /** 
         * @private
         * @type {import('../../config/configManager.js').ConfigManager} 
         */
        // [ä¿®æ­£] ä½¿ç”¨æ­£ç¡®çš„å•ä¾‹è·å–æ–¹å¼
        this.configManager = ConfigManager.instance();
        if (!this.configManager) {
            throw new Error("LLMService æ— æ³•åˆå§‹åŒ–ï¼šConfigManager å°šæœªè¢«åˆ›å»ºã€‚");
        }

        /**
         * @private
         * @type {Map<string, LLMClient>}
         */
        this.clientCache = new Map();
        
        this._listenForConfigChanges();

        instance = this; // ç¡®ä¿ instance è¢«èµ‹å€¼
    }

    /**
     * è·å– LLMService çš„å…¨å±€å•ä¾‹å®ä¾‹ã€‚
     * @returns {LLMService}
     */
    static getInstance() {
        if (!instance) {
            instance = new LLMService();
        }
        return instance;
    }

    /**
     * æ ¹æ® connectionId è·å–ä¸€ä¸ªé…ç½®å¥½çš„ LLMClient å®ä¾‹ã€‚
     * @param {string} connectionId
     * @returns {LLMClient} // [ä¿®æ­£] è¿”å› LLMClient è€Œä¸æ˜¯ Promiseï¼Œå› ä¸ºé…ç½®æ˜¯åŒæ­¥è·å–çš„
     * @throws {Error} å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„è¿æ¥é…ç½®ã€‚
     */
    getClient(connectionId) {
        if (this.clientCache.has(connectionId)) {
            return this.clientCache.get(connectionId);
        }

        // [ä¿®æ­£] ä½¿ç”¨æ­£ç¡®çš„å…¬å…±æ¥å£è·å–è¿æ¥é…ç½®
        const connectionConfig = this.configManager.getLLMConnection(connectionId);

        if (!connectionConfig) {
            throw new Error(`[LLMService] IDä¸º '${connectionId}' çš„è¿æ¥é…ç½®æœªæ‰¾åˆ°ã€‚`);
        }
        
        // å°†å­˜å‚¨çš„é…ç½®è½¬æ¢ä¸º LLMClient éœ€è¦çš„è¿è¡Œæ—¶é…ç½®
        const clientRuntimeConfig = {
            provider: connectionConfig.provider,
            apiKey: connectionConfig.apiKey,
            apiBaseUrl: connectionConfig.baseURL, // å±æ€§åä¹Ÿéœ€è¦åŒ¹é… LLMClient
            // è¿è¡Œæ—¶å‚æ•°å¯ä»¥åŠ¨æ€æ·»åŠ 
            referer: typeof window !== 'undefined' ? window.location.href : '',
            title: typeof document !== 'undefined' ? document.title : 'LLM App',
        };

        console.log(`[LLMService] æ­£åœ¨ä¸º connectionId '${connectionId}' åˆ›å»ºæ–°çš„ LLMClient å®ä¾‹ã€‚`);
        const client = new LLMClient(clientRuntimeConfig);
        this.clientCache.set(connectionId, client);

        return client;
    }

    /**
     * æ¸…é™¤å®¢æˆ·ç«¯ç¼“å­˜ã€‚å½“è¿æ¥ä¿¡æ¯æ›´æ–°æ—¶ç”±äº‹ä»¶å¤„ç†å™¨è‡ªåŠ¨è°ƒç”¨ã€‚
     * @param {string} [connectionId] - å¦‚æœæä¾›ï¼Œåˆ™åªæ¸…é™¤æŒ‡å®šIDçš„å®¢æˆ·ç«¯ã€‚å¦åˆ™æ¸…é™¤æ‰€æœ‰ã€‚
     * @private
     */
    _clearCache(connectionId) {
        if (connectionId) {
            if (this.clientCache.delete(connectionId)) {
                 console.log(`[LLMService] å·²æ¸…é™¤ Connection '${connectionId}' çš„å®¢æˆ·ç«¯ç¼“å­˜ã€‚`);
            }
        } else {
            this.clientCache.clear();
            console.log('[LLMService] å·²æ¸…é™¤æ‰€æœ‰å®¢æˆ·ç«¯ç¼“å­˜ã€‚');
        }
    }
    
    /**
     * [ä¿®æ­£] è®¢é˜…æ­£ç¡®çš„äº‹ä»¶å¹¶å¤„ç†æ–°çš„è½½è·ç»“æ„
     * @private
     * @description
     * è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å“åº”å¼æ”¹è¿›ã€‚å®ƒå°† LLMService ä»ä¸€ä¸ªè¢«åŠ¨çš„æœåŠ¡æä¾›è€…ï¼Œ
     * è½¬å˜ä¸ºä¸€ä¸ªèƒ½å¤Ÿå“åº”ç³»ç»ŸçŠ¶æ€å˜åŒ–çš„ä¸»åŠ¨ç®¡ç†è€…ã€‚
     */
    _listenForConfigChanges() {
        // ç›‘å¬æˆ‘ä»¬å®šä¹‰çš„é€šç”¨æ›´æ–°äº‹ä»¶
        this.configManager.on(EVENTS.LLM_CONFIG_UPDATED, (payload) => {
            console.log('[LLMService] æ£€æµ‹åˆ° LLM é…ç½®å·²æ›´æ–°ã€‚', payload);
            
            // å½“ä¸€ä¸ªè¿æ¥è¢«ä¿®æ”¹æˆ–åˆ é™¤æ—¶ï¼Œæ¸…é™¤å…¶ç¼“å­˜
            if (payload.type === 'connection') {
                this._clearCache(payload.id);
            }
            // å¦‚æœæ˜¯æ›´å¹¿æ³›çš„å˜æ›´æˆ–ä¸ç¡®å®šï¼Œå¯ä»¥æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            // else { this._clearCache(); }
        });
    }
}
/**
 * @file src/llm/core/api.js
 * @description Provides high-level, standalone functions for interacting with the core library.
 */

import { LLMClient } from './client.js';
import { LLMError } from './errors.js';
import { LLM_PROVIDER_DEFAULTS } from '../../common/configData.js'; // âœ… æ·»åŠ è¿™è¡Œ

/**
 * Tests a given LLM provider connection configuration by making a lightweight API call.
 * This function is designed to be used independently, for example, in a settings UI.
 *
 * @param {object} connectionConfig - The connection configuration to test.
 * @param {string} connectionConfig.provider - The provider name (e.g., 'openai').
 * @param {string} connectionConfig.apiKey - The API key.
 * @param {string} [connectionConfig.baseURL] - The optional base URL for the API.
 * @returns {Promise<{success: boolean, message: string}>} A result object indicating success or failure.
 */
export async function testLLMConnection(connectionConfig) {
    if (!connectionConfig || !connectionConfig.provider || !connectionConfig.apiKey) {
        return { success: false, message: "Provider and API Key are required." };
    }

    try {
        // 1. Create a temporary client with the provided config.
        const client = new LLMClient({
            provider: connectionConfig.provider,
            apiKey: connectionConfig.apiKey,
            apiBaseUrl: connectionConfig.baseURL,
        });

        // âœ… ä¿®å¤ï¼šç¡®ä¿æ€»æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„ model
        const modelToUse = connectionConfig.model || 
                          connectionConfig.availableModels?.[0]?.id ||
                          LLM_PROVIDER_DEFAULTS[connectionConfig.provider]?.models?.[0]?.id ||
                          'gpt-4o'; // æœ€åçš„å…œåº•é€‰é¡¹

        console.log(`[Test Connection] Using model: ${modelToUse} for provider: ${connectionConfig.provider}`);

        // âœ… ä¿®å¤ï¼šåˆ›å»º AbortSignalï¼Œä½†é€šè¿‡æ­£ç¡®çš„æ–¹å¼ä¼ é€’
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 15000);

        try {
            const response = await client.chat.create({
                messages: [{ role: 'user', content: 'hello' }],
                model: modelToUse,
                max_tokens: 1, // âœ… ç›´æ¥ä½œä¸ºé¡¶å±‚å‚æ•°
                signal: controller.signal // âœ… signal ä½œä¸ºé¡¶å±‚å‚æ•°ï¼Œä¸åœ¨ options é‡Œ
            });
            
            clearTimeout(timeoutId);
            
            if (response.choices && response.choices.length > 0) {
                return { success: true, message: 'Connection successful!' };
            } else {
                return { success: false, message: 'Connection succeeded, but response was empty.' };
            }
        } catch (error) {
            clearTimeout(timeoutId);
            throw error;
        }

    } catch (error) {
        // 4. If an error occurs, format it into a user-friendly message.
        console.error("Connection test failed:", error);
        if (error instanceof LLMError) {
            let message = `API Error: ${error.message}`;
            if (error.statusCode) {
                message += ` (HTTP Status: ${error.statusCode})`;
            }
            return { success: false, message };
        }
        if (error.name === 'AbortError') {
            return { success: false, message: 'Connection failed: The request timed out.' };
        }
        return { success: false, message: `An unexpected error occurred: ${error.message}` };
    }
}/**
 * @file src/llm/core/chain.js
 * @description Provides a simple sequential chaining mechanism for LLM calls.
 */

import { LLMClient } from './client.js';

/**
 * A simple prompt template function.
 * @private
 */
function templatize(template, variables) {
    return template.replace(/{(\w+)}/g, (_, key) => {
        if (variables[key] === undefined) {
            throw new Error(`Template variable "${key}" not found in context.`);
        }
        return variables[key];
    });
}

export class LLMChain {
    /**
     * @param {LLMClient} client - An instance of LLMClient to execute steps.
     */
    constructor(client) {
        if (!client || typeof client.chat?.create !== 'function') {
            throw new Error('LLMChain requires a valid instance of LLMClient.');
        }
        this.client = client;
        this.steps = [];
    }

    /**
     * Adds a processing step to the chain.
     * @param {object} stepConfig
     * @param {string} stepConfig.promptTemplate - The prompt template for this step.
     * @param {string[]} stepConfig.inputVariables - Variables from the context to use in the template.
     * @param {string} stepConfig.outputVariable - The key under which to save the LLM output in the context.
     * @param {object} [llmConfig] - Additional parameters for the client.chat.create call (e.g., model, temperature).
     * @returns {LLMChain} - The chain instance for fluent chaining.
     */
    add(stepConfig, llmConfig = {}) {
        const { promptTemplate, inputVariables, outputVariable } = stepConfig;
        if (!promptTemplate || !inputVariables || !outputVariable) {
            throw new Error("Step config must include 'promptTemplate', 'inputVariables', and 'outputVariable'.");
        }
        this.steps.push({ ...stepConfig, llmConfig });
        return this;
    }

    /**
     * Executes the chain with an initial context.
     * @param {object} initialContext - The initial key-value pairs for the run.
     * @returns {Promise<object>} - The final context containing inputs and all generated outputs.
     */
    async run(initialContext = {}) {
        let context = { ...initialContext };

        for (const step of this.steps) {
            const promptInput = {};
            step.inputVariables.forEach(key => {
                promptInput[key] = context[key];
            });

            const formattedPrompt = templatize(step.promptTemplate, promptInput);
            
            const response = await this.client.chat.create({
                messages: [{ role: 'user', content: formattedPrompt }],
                ...step.llmConfig
            });

            const result = response.choices[0].message.content;
            context[step.outputVariable] = result;
        }

        return context;
    }
}
/**
 * @file src/llm/core/index.js
 * @description Main entry point for the llm-fusion-kit library.
 */

export { LLMClient } from './client.js';
export { LLMChain } from './chain.js';
export { LLMService } from './LLMService.js'; // å‡è®¾ LLMService çš„è·¯å¾„
export {testLLMConnection} from './api.js';/**
 * @file src/llm/core/utils/file-processor.js
 * @description Handles conversion of various attachment sources to base64 data URI.
 * This is an isomorphic utility, designed to work in both Node.js and Browser environments.
 */

/**
 * Converts a value to a Buffer in a platform-agnostic way.
 * @param {Blob | ArrayBuffer} value - The value to convert.
 * @returns {Promise<Buffer>}
 */
async function toBuffer(value) {
    if (typeof Buffer !== 'undefined' && value instanceof Buffer) {
        return value;
    }
    if (typeof Blob !== 'undefined' && value instanceof Blob) {
        const arrayBuffer = await value.arrayBuffer();
        return Buffer.from(arrayBuffer);
    }
    if (value instanceof ArrayBuffer) {
        return Buffer.from(value);
    }
    throw new Error('Unsupported type for buffer conversion.');
}


/**
 * Processes various attachment sources (File, Blob, Buffer, URL, base64 string)
 * into a uniform format required by LLM APIs.
 *
 * @param {File | Blob | Buffer | string} source - The attachment source.
 * @param {string} [mimeType='application/octet-stream'] - Default MIME type if not detectable.
 * @returns {Promise<{mimeType: string, base64: string}>} - The processed attachment data.
 */
export async function processAttachment(source, mimeType) {
    // 1. Handle string inputs (base64 URI, http URL, or raw base64)
    if (typeof source === 'string') {
        if (source.startsWith('data:')) {
            const parts = source.match(/^data:(.+?);base64,(.+)$/);
            if (!parts) throw new Error('Invalid base64 data URI format.');
            return { mimeType: parts[1], base64: parts[2] };
        }
        if (source.startsWith('http')) {
            const response = await fetch(source);
            if (!response.ok) throw new Error(`Failed to fetch image from ${source}: ${response.statusText}`);
            const blob = await response.blob();
            const buffer = await blob.arrayBuffer();
            const base64 = (typeof Buffer !== 'undefined' ? Buffer.from(buffer) : btoa(String.fromCharCode(...new Uint8Array(buffer)))).toString('base64');
            return { mimeType: response.headers.get('content-type') || blob.type || mimeType, base64 };
        }
        // Assume it's a raw base64 string
        return { mimeType: mimeType || 'application/octet-stream', base64: source };
    }

    // 2. Handle Blob or File (Browser environment)
    if (typeof Blob !== 'undefined' && source instanceof Blob) {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onload = () => {
                const result = reader.result;
                const base64 = result.substring(result.indexOf(',') + 1);
                resolve({ mimeType: source.type || mimeType, base64 });
            };
            reader.onerror = (error) => reject(error);
            reader.readAsDataURL(source);
        });
    }

    // 3. Handle Buffer (Node.js environment)
    if (typeof Buffer !== 'undefined' && source instanceof Buffer) {
      return { mimeType: mimeType || 'application/octet-stream', base64: source.toString('base64') };
    }

    throw new Error('Unsupported attachment source type. Please provide a URL, base64 string, File/Blob, or Buffer.');
}
/* file: src/llm/chat/styles.css */

.chat-editor-wrapper {
    display: flex;
    flex-direction: column;
    height: 100%;
    background: #f9fafb;
}

.chat-editor-main {
    flex-grow: 1;
    overflow-y: auto;
    padding: 20px;
    display: flex;
    flex-direction: column;
    gap: 16px;
    scroll-behavior: smooth;
}

/* æ¶ˆæ¯å—æ ·å¼æ”¹è¿› */
.chat-message-block {
    border-radius: 12px;
    overflow: visible;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: all 0.2s;
    animation: slideIn 0.3s ease;
    flex-shrink: 0;
    min-height: 100px;
}

@keyframes slideIn {
    from {
        opacity: 0;
        transform: translateY(20px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

.chat-message-block:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

/* ç”¨æˆ·æ¶ˆæ¯ */
.chat-message-block.user {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
}

/* AI å›å¤ */
.chat-message-block.assistant {
    background: white;
    border: 2px solid #e5e7eb;
    position: relative;
}

/* ä¸ºä¸åŒ Agent æ·»åŠ ä¸åŒçš„å¼ºè°ƒè‰² */
.chat-message-block[data-agent]::before {
    content: '';
    position: absolute;
    left: 0;
    top: 0;
    bottom: 0;
    width: 4px;
    background: var(--agent-color, #3b82f6);
}

/* Agent å›¾æ ‡æ˜¾ç¤º */
.chat-message-block .agent-icon {
    font-size: 20px;
    margin-right: 8px;
}

/* è¾“å…¥åŒºåŸŸæ”¹è¿› */
.chat-input-bar {
    background: white;
    border-top: 1px solid #e5e7eb;
    padding: 16px 20px;
    display: flex;
    gap: 12px;
    align-items: flex-end;
    box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.05);
}

.chat-agent-select {
    min-width: 180px;
    padding: 10px;
    border: 2px solid #e5e7eb;
    border-radius: 8px;
    font-size: 14px;
    background: white;
    cursor: pointer;
    transition: border-color 0.2s;
    flex-shrink: 0;
}

.chat-agent-select:focus {
    outline: none;
    border-color: #3b82f6;
}

.chat-add-agent-btn {
    padding: 10px 14px;
    background: #10b981;
    color: white;
    border: none;
    border-radius: 8px;
    font-size: 18px;
    font-weight: bold;
    cursor: pointer;
    transition: all 0.2s;
    flex-shrink: 0;
}

.chat-add-agent-btn:hover {
    background: #059669;
    transform: scale(1.05);
}

/* è¾“å…¥æ¡†å®¹å™¨ */
.chat-input-wrapper {
    flex: 1;
    display: flex;
    flex-direction: column;
    position: relative;
    min-width: 0;
}

.chat-prompt-input {
    width: 100%;
    padding: 12px 60px 12px 12px;
    border: 2px solid #e5e7eb;
    border-radius: 8px;
    font-size: 14px;
    resize: none;
    min-height: 44px;
    max-height: 200px;
    transition: border-color 0.2s;
    font-family: inherit;
    box-sizing: border-box;
}

.chat-prompt-input:focus {
    outline: none;
    border-color: #3b82f6;
}

/* å­—æ•°ç»Ÿè®¡ */
.char-count {
    position: absolute;
    bottom: 12px;
    right: 12px;
    font-size: 11px;
    color: #9ca3af;
    pointer-events: none;
}

.chat-send-btn {
    padding: 12px 24px;
    background: #3b82f6;
    color: white;
    border: none;
    border-radius: 8px;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.2s;
    flex-shrink: 0;
    white-space: nowrap;
}

.chat-send-btn:hover:not(:disabled) {
    background: #2563eb;
    transform: translateY(-1px);
    box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
}

.chat-send-btn:disabled {
    background: #d1d5db;
    cursor: not-allowed;
}

/* +++ NEW STYLES for System Message Block +++ */
.chat-message-block.system-message {
    background-color: #f8f9fa; /* A light grey background */
    border-left: 4px solid #6c757d; /* A subtle grey left border */
    padding: 12px 16px;
    margin: 8px 0;
    border-radius: 4px;
}

.system-message-header {
    display: flex;
    align-items: center;
    font-size: 0.9em;
    font-weight: bold;
    color: #495057; /* Darker grey for text */
    margin-bottom: 8px;
}

.system-message-icon {
    margin-right: 8px;
}

.system-message-title {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
}

.system-message-content {
    white-space: pre-wrap;
    word-wrap: break-word;
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    font-size: 0.85em;
    background-color: #e9ecef; /* A slightly darker grey for the code block */
    padding: 8px 12px;
    border-radius: 4px;
    color: #343a40;
    max-height: 200px; /* Limit height for very long prompts */
    overflow-y: auto;
}

/* âœ… æ–°å¢ï¼šGroup å“åº”æ ·å¼ */
.chat-group-parallel {
    border-left: 3px solid #8b5cf6;
    background: linear-gradient(to right, rgba(139, 92, 246, 0.05), transparent);
}

.chat-group-header {
    padding: 12px 16px;
    background: rgba(139, 92, 246, 0.1);
    border-bottom: 1px solid rgba(139, 92, 246, 0.2);
    font-weight: 600;
    display: flex;
    align-items: center;
    gap: 8px;
}

.group-icon {
    font-size: 1.2em;
}

.chat-parallel-responses {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 16px;
    padding: 16px;
}

.chat-parallel-response-item {
    border: 1px solid rgba(139, 92, 246, 0.2);
    border-radius: 8px;
    overflow: hidden;
    background: white;
}

.parallel-agent-header {
    padding: 8px 12px;
    background: rgba(139, 92, 246, 0.1);
    font-weight: 500;
    font-size: 0.9em;
    color: #6d28d9;
}

.parallel-agent-content {
    padding: 12px;
    margin: 0;
    font-size: 0.9em;
    line-height: 1.6;
    white-space: pre-wrap;
    word-wrap: break-word;
    max-height: 400px;
    overflow-y: auto;
}

/* âœ… æ–°å¢ï¼šå¹¶è¡Œå“åº”å—æ ·å¼ */
.chat-parallel-block {
    border-left: 3px solid #10b981;
    background: linear-gradient(to right, rgba(16, 185, 129, 0.05), transparent);
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 768px) {
    .chat-input-bar {
        padding: 12px;
        gap: 8px;
    }
    
    .chat-agent-select {
        min-width: 120px;
        font-size: 13px;
    }
    
    .chat-send-btn {
        padding: 12px 16px;
    }
    .chat-parallel-responses {
        grid-template-columns: 1fr;
    }
}
/**
 * @file src/llm/core/providers/openai.js
 * @description Adapter for OpenAI and OpenAI-compatible APIs (DeepSeek, OpenRouter).
 */
import { BaseProvider } from './base.js';
import { LLMError } from '../errors.js';
import { processAttachment } from '../utils/file-processor.js';
// +++ å¯¼å…¥å…±äº«æ•°æ®
import { LLM_PROVIDER_DEFAULTS } from '../../../common/configData.js';

// --- åˆ é™¤æ—§çš„ PROVIDER_URLS å¸¸é‡ ---

export class OpenAICompatibleProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = config.provider;
        const defaultBaseURL = LLM_PROVIDER_DEFAULTS[this.providerName]?.baseURL;
        this.apiBaseUrl = config.apiBaseUrl || defaultBaseURL;

        // âœ… æ·»åŠ é»˜è®¤æ¨¡å‹å¤„ç†
        const providerDefaults = LLM_PROVIDER_DEFAULTS[this.providerName];
        this.defaultModel = this.model || providerDefaults?.models?.[0]?.id || 'gpt-4o';

        if (!this.apiBaseUrl) {
            throw new Error(`Base URL for provider '${this.providerName}' is not defined.`);
        }
    }

    async _prepareMessages(messages) {
        const processedMessages = [];
        for (const message of messages) {
            if (!Array.isArray(message.content)) {
                processedMessages.push(message);
                continue;
            }

            const newContent = [];
            for (const part of message.content) {
                if (part.type === 'text') {
                    newContent.push(part);
                } else if (part.type === 'image_url') {
                    const source = part.image_url.url;
                    const { mimeType, base64 } = await processAttachment(source);
                    newContent.push({
                        type: 'image_url',
                        image_url: {
                            url: `data:${mimeType};base64,${base64}`
                        }
                    });
                }
            }
            processedMessages.push({ ...message, content: newContent });
        }
        return processedMessages;
    }

    async _fetchAPI(body, signal) { // +++ Add signal parameter
        const headers = {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.apiKey}`,
        };
        if (this.providerName === 'openrouter') {
            headers['HTTP-Referer'] = this.config.referer || 'http://localhost:3000';
            headers['X-Title'] = this.config.title || 'LLM Fusion Kit';
        }

        const response = await fetch(this.apiBaseUrl, {
            method: 'POST',
            headers,
            body: JSON.stringify(body),
            signal, // +++ Use the signal here
        });

        if (!response.ok) {
            // --- START: MODIFIED ERROR HANDLING ---
            let errorData = null;
            let errorMessage = response.statusText; // Default to the HTTP status text

            try {
                // Try to parse the error body as JSON
                errorData = await response.json();
                
                // Intelligently find the error message from various possible structures
                errorMessage =
                    errorData?.error?.message || // Standard OpenAI format: { error: { message: '...' } }
                    errorData?.message ||         // Simple format: { message: '...' }
                    errorData?.error ||           // Another simple format: { error: '...' }
                    JSON.stringify(errorData);    // Fallback to the full error object
            } catch (e) {
                // If JSON parsing fails, the body might be plain text.
                // We already have the statusText as a fallback, which is sufficient.
                console.error("Failed to parse error response as JSON.", e);
            }

            throw new LLMError(errorMessage, {
                cause: errorData, // Attach the full error data for debugging
                provider: this.providerName,
                statusCode: response.status,
            });
        }
        return response;
    }

    async create(params, signal) { // +++ Add signal parameter
        const messages = await this._prepareMessages(params.messages);
        
        // âœ… ç¡®ä¿ model ä¸ä¸ºç©º
        const modelToUse = params.model || this.defaultModel;
        
        if (!modelToUse) {
            throw new LLMError('No model specified and no default model available', {
                provider: this.providerName,
                statusCode: 400
            });
        }
        
        const body = {
            model: modelToUse, // âœ… ä½¿ç”¨å¤„ç†åçš„æ¨¡å‹åç§°
            messages,
            stream: false,
            // Conditionally add tool parameters if they exist
            ...(params.tools && { tools: params.tools }),
            ...(params.tool_choice && { tool_choice: params.tool_choice }),
            ...(params.options || {}),
        };
        
        console.log(`[OpenAI Provider] Using model: ${modelToUse}`);
        
        const response = await this._fetchAPI(body, signal);
        return response.json();
    }

    async* stream(params, signal) { // +++ Add signal parameter
            // --- SIMULATION LOGIC FOR DEMO ---
        const modelToUse = params.model || this.defaultModel;
        
        if (!modelToUse) {
            throw new LLMError('No model specified and no default model available', {
                provider: this.providerName,
                statusCode: 400
            });
        }
    
        // In a real implementation, you would not add this block.
        // Instead, you'd parse the actual stream from the provider which might contain
        // special tokens or tags (like <thinking>...</thinking>) and yield them as thinking chunks.
        if (params.include_thinking && params.tools) {
            yield { choices: [{ delta: { thinking: "User asked a question. I should check if any tools can help." } }] };
            await new Promise(res => setTimeout(res, 300)); // Simulate processing delay
            yield { choices: [{ delta: { thinking: "The `get_current_weather` tool seems relevant." } }] };
            await new Promise(res => setTimeout(res, 300));
        }
        // --- END OF SIMULATION LOGIC ---

        const messages = await this._prepareMessages(params.messages);
        const body = {
            model: modelToUse,
            messages,
            stream: true,
            // Conditionally add tool parameters if they exist
            ...(params.tools && { tools: params.tools }),
            ...(params.tool_choice && { tool_choice: params.tool_choice }),
            ...(params.options || {}),
        };
        
        const response = await this._fetchAPI(body, signal); // +++ Pass signal

        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop(); // Keep potentially incomplete line

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const data = line.substring(6).trim();
                    if (data === '[DONE]') {
                        return;
                    }
                    try {
                        // 2. åœ¨è¿™é‡ŒåŠ å…¥çœŸå®è§£æé€»è¾‘
                        const parsedData = JSON.parse(data);
                        
        const delta = parsedData.choices?.[0]?.delta;
        
        if (!delta) {
            continue;
        }
        
        
        let hasYielded = false;
        
        if (params.include_thinking && delta.reasoning_content) {
            yield { choices: [{ delta: { thinking: delta.reasoning_content } }] };
            hasYielded = true;
        }
        
        if (delta.content) {

            yield {
                choices: [{
                    delta: { content: delta.content },
                    finish_reason: parsedData.choices[0]?.finish_reason || null
                }]
            };
            hasYielded = true;
        }

                        // 3. æ­£å¸¸ yield åŸå§‹æ•°æ®ï¼Œæˆ–è€…åª yield content éƒ¨åˆ†
                    if (!hasYielded && (delta.tool_calls || delta.function_call)) {
                        yield parsedData;
                    }

                    } catch (e) {
                        console.error('Failed to parse stream chunk:', data);
                    }
                }
            }
        }
    }
}
/**
 * @file src/llm/core/client.js
 * @description The main client class and entry point for the library.
 */

import { OpenAICompatibleProvider } from './providers/openai.js';
import { AnthropicProvider } from './providers/anthropic.js';
import { GeminiProvider } from './providers/gemini.js';
import { LLM_PROVIDER_DEFAULTS } from '../../common/configData.js';


// +++ 2. ç”¨ä¸€ä¸ªæ›´å°çš„ã€åŸºäºå®ç°ç±»å‹çš„æ˜ å°„æ›¿æ¢ PROVIDER_MAP +++
/**
 * Maps an implementation type string to its corresponding provider class.
 * This allows multiple providers in LLM_PROVIDER_DEFAULTS to share a single implementation.
 * @type {Object.<string, import('./providers/base.js').BaseProvider>}
 */
const PROVIDER_IMPLEMENTATIONS = {
    'openai-compatible': OpenAICompatibleProvider,
    'anthropic': AnthropicProvider,
    'gemini': GeminiProvider,
};

/**
 * Factory function to create a provider instance based on configuration.
 * @param {object} config - The client configuration.
 * @returns {import('./providers/base.js').BaseProvider} An instance of a provider adapter.
 */
function createProvider(config) {
    const providerId = config.provider;

    // Step 1: Get the provider's metadata from our single source of truth.
    const providerInfo = LLM_PROVIDER_DEFAULTS[providerId];
    if (!providerInfo) {
        throw new Error(`Provider '${providerId}' is not defined in LLM_PROVIDER_DEFAULTS.`);
    }

    // Step 2: Determine which implementation class to use.
    const implementationType = providerInfo.implementation;
    if (!implementationType) {
        throw new Error(`Provider '${providerId}' is missing the 'implementation' property in LLM_PROVIDER_DEFAULTS.`);
    }

    // Step 3: Find the corresponding class from our implementation map.
    const ProviderClass = PROVIDER_IMPLEMENTATIONS[implementationType];
    if (!ProviderClass) {
        throw new Error(`No provider implementation found for type '${implementationType}'. Supported types are: ${Object.keys(PROVIDER_IMPLEMENTATIONS).join(', ')}`);
    }
    return new ProviderClass(config);
}

export class LLMClient {
    /**
     * Initializes a new LLM client.
     * @param {object} config - The client configuration.
     * @param {string} config.provider - The name of the LLM provider.
     * @param {string} config.apiKey - The API key for the selected provider.
     * @param {string} [config.model] - The default model to use.
     * @param {object} [config.hooks] - Lifecycle hooks for requests.
     * @param {function(object): Promise<object>} [config.hooks.beforeRequest]
     * @param {function(object): Promise<object>} [config.hooks.afterResponse]
     * @param {function(Error, object): Promise<void>} [config.hooks.onError]
     */
    constructor(config) {
        if (!config || !config.provider || !config.apiKey) {
            throw new Error('Configuration object with `provider` and `apiKey` is required.');
        }
        this.provider = createProvider(config);
        this.hooks = config.hooks || {};
        
        this.chat = {
            create: this.createChatCompletion.bind(this)
        };
    }

    /**
     * Executes the provider call, wrapping it with hooks.
     * @private
     */
    async _execute(params) {
        let finalParams = params;
        try {
            if (this.hooks.beforeRequest) {
                finalParams = await this.hooks.beforeRequest(params);
            }
            
            // Pass the signal down to the provider call
            const response = params.stream 
                ? this.provider.stream(finalParams, params.options?.signal) 
                : await this.provider.create(finalParams, params.options?.signal);

            // Note: afterResponse hook does not apply to streams in this implementation
            // as it would require wrapping the async generator.
            if (this.hooks.afterResponse && !params.stream) {
                 return await this.hooks.afterResponse(response);
            }
            return response;

        } catch (error) {
            if (this.hooks.onError) {
                await this.hooks.onError(error, finalParams);
            }
            throw error; // Re-throw the error after the hook
        }
    }

    /**
     * Creates a chat completion. This is the primary method for interacting with LLMs.
     * @param {object} params
     * @param {Array<object>} params.messages - The conversation history.
     * @param {string} [params.model] - Overrides the client's default model.
     * @param {boolean} [params.stream=false] - If true, returns an async generator.
     * @param {boolean} [params.include_thinking=false] - If true, requests the model's thinking process.
     * @param {number} [params.temperature] - The sampling temperature.
     * @param {number} [params.max_tokens] - The maximum number of tokens to generate.
     * @param {number} [params.top_p] - The nucleus sampling probability.
     * @param {Array<object>} [params.tools] - A list of tools the model may call.
     * @param {object | string} [params.tool_choice] - Controls which tool is called.
     * @param {object} [params.options] - Provider-specific or less common options.
     * @returns {Promise<object> | AsyncGenerator<object>}
     */
    createChatCompletion({
        messages,
        model,
        stream = false,
        include_thinking = false, // NEW PARAMETER
        temperature,
        max_tokens,
        top_p,
        tools,
        tool_choice,
    signal, // âœ… æ–°å¢ï¼šsignal ä½œä¸ºé¡¶å±‚å‚æ•°
        options = {}
    }) {
        const providerParams = {
            messages,
            model,
            stream,
            include_thinking, // Pass it down to the provider
            tools,
            tool_choice,
        signal, // âœ… ä¼ é€’ signal
            // This allows users to use top-level args for convenience
            // while still allowing overrides via the options object.
            options: {
                temperature,
                max_tokens,
                top_p,
                ...options,
            }
        };

        // Remove undefined top-level keys so they don't override provider defaults
        Object.keys(providerParams.options).forEach(key => 
            providerParams.options[key] === undefined && delete providerParams.options[key]
        );

        return this._execute(providerParams);
    }
}
/**
 * @file src/llm/core/providers/base.js
 * @description Defines the abstract base class for all provider adapters.
 */

export class BaseProvider {
    /**
     * @param {object} config - The provider configuration.
     * @param {string} config.apiKey - The API key for the provider.
     * @param {string} config.model - The default model to use.
     */
    constructor(config) {
        if (this.constructor === BaseProvider) {
            throw new Error("Abstract class 'BaseProvider' cannot be instantiated directly.");
        }
        this.apiKey = config.apiKey;
        this.model = config.model;
        this.config = config;
    }

    /**
     * Creates a standard (non-streaming) chat completion.
     * @abstract
     * @param {object} params - The standardized request parameters from LLMClient.
     * @param {boolean} params.include_thinking - Flag to include thinking process.
     * @returns {Promise<object>}
     */
    async create(params) {
        throw new Error("Method 'create()' must be implemented by subclasses.");
    }

    /**
     * Creates a streaming chat completion.
     * @abstract
     * @param {object} params - The standardized request parameters from LLMClient.
     * @param {boolean} params.include_thinking - Flag to include thinking process.
     * @returns {AsyncGenerator<object>}
     */
    async* stream(params) {
        throw new Error("Method 'stream()' must be implemented by subclasses.");
    }
}
/**
 * @file src/llm/core/providers/anthropic.js
 * @description Adapter for Anthropic Claude API with its specific message format.
 */
import { BaseProvider } from './base.js';
import { LLMError } from '../errors.js';
import { processAttachment } from '../utils/file-processor.js';
import { LLM_PROVIDER_DEFAULTS } from '../../../common/configData.js';

export class AnthropicProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = config.provider;
        const defaultBaseURL = LLM_PROVIDER_DEFAULTS[this.providerName]?.baseURL;
        this.apiBaseUrl = config.apiBaseUrl || defaultBaseURL;
        this.apiVersion = '2023-06-01';

        if (!this.apiBaseUrl) {
            throw new Error(`Base URL for provider '${this.providerName}' is not defined.`);
        }
    }

    /**
     * Transform OpenAI-style messages to Anthropic format
     */
    _transformMessages(messages) {
        const systemMessages = messages.filter(m => m.role === 'system');
        const conversationMessages = messages.filter(m => m.role !== 'system');
        
        const systemPrompt = systemMessages.map(m => m.content).join('\n');
        
        return {
            system: systemPrompt || undefined,
            messages: conversationMessages.map(msg => ({
                role: msg.role === 'assistant' ? 'assistant' : 'user',
                content: this._transformContent(msg.content)
            }))
        };
    }

    _transformContent(content) {
        if (typeof content === 'string') {
            return content;
        }
        
        if (Array.isArray(content)) {
            return content.map(part => {
                if (part.type === 'text') {
                    return { type: 'text', text: part.text };
                } else if (part.type === 'image_url') {
                    return {
                        type: 'image',
                        source: {
                            type: 'base64',
                            media_type: 'image/jpeg',
                            data: part.image_url.url.split(',')[1]
                        }
                    };
                }
                return part;
            });
        }
        
        return content;
    }

    async _prepareMessages(messages) {
        const processedMessages = [];
        for (const message of messages) {
            if (!Array.isArray(message.content)) {
                processedMessages.push(message);
                continue;
            }

            const newContent = [];
            for (const part of message.content) {
                if (part.type === 'text') {
                    newContent.push(part);
                } else if (part.type === 'image_url') {
                    const source = part.image_url.url;
                    const { mimeType, base64 } = await processAttachment(source);
                    newContent.push({
                        type: 'image_url',
                        image_url: {
                            url: `data:${mimeType};base64,${base64}`
                        }
                    });
                }
            }
            processedMessages.push({ ...message, content: newContent });
        }
        return processedMessages;
    }

    async _fetchAPI(body, signal) {
        const headers = {
            'Content-Type': 'application/json',
            'x-api-key': this.apiKey,
            'anthropic-version': this.apiVersion,
        };

        const response = await fetch(this.apiBaseUrl, {
            method: 'POST',
            headers,
            body: JSON.stringify(body),
            signal,
        });

        if (!response.ok) {
            const errorData = await response.json().catch(() => ({ 
                error: { message: response.statusText } 
            }));
            throw new LLMError(errorData.error?.message || 'Anthropic API error', {
                cause: errorData,
                provider: this.providerName,
                statusCode: response.status,
            });
        }
        return response;
    }

    async create(params, signal) {
        const messages = await this._prepareMessages(params.messages);
        const transformed = this._transformMessages(messages);
        
        const body = {
            model: params.model || this.model,
            max_tokens: params.options?.max_tokens || 1024,
            ...transformed,
            stream: false,
            temperature: params.options?.temperature,
            top_p: params.options?.top_p,
        };
        
        // Remove undefined values
        Object.keys(body).forEach(key => 
            body[key] === undefined && delete body[key]
        );
        
        const response = await this._fetchAPI(body, signal);
        const data = await response.json();
        
        // Transform Anthropic response to OpenAI format
        return {
            choices: [{
                message: {
                    role: 'assistant',
                    content: data.content[0]?.text || ''
                },
                finish_reason: data.stop_reason
            }],
            usage: data.usage,
            model: data.model
        };
    }

    async* stream(params, signal) {
        const messages = await this._prepareMessages(params.messages);
        const transformed = this._transformMessages(messages);
        
        const body = {
            model: params.model || this.model,
            max_tokens: params.options?.max_tokens || 1024,
            ...transformed,
            stream: true,
            temperature: params.options?.temperature,
            top_p: params.options?.top_p,
        };
        
        Object.keys(body).forEach(key => 
            body[key] === undefined && delete body[key]
        );
        
        const response = await this._fetchAPI(body, signal);
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop();

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const data = line.substring(6).trim();
                    
                    try {
                        const parsedData = JSON.parse(data);
                        
                        if (parsedData.type === 'content_block_delta') {
                            const content = parsedData.delta?.text || '';
                            if (content) {
                                yield {
                                    choices: [{
                                        delta: { content },
                                        finish_reason: null
                                    }]
                                };
                            }
                        } else if (parsedData.type === 'message_stop') {
                            yield {
                                choices: [{
                                    delta: {},
                                    finish_reason: 'stop'
                                }]
                            };
                        }
                    } catch (e) {
                        console.error('Failed to parse Anthropic stream chunk:', data);
                    }
                }
            }
        }
    }
}
/**
 * @file src/llm/core/providers/gemini.js
 * @description Adapter for Google Gemini API with its specific format.
 */
import { BaseProvider } from './base.js';
import { LLMError } from '../errors.js';
import { processAttachment } from '../utils/file-processor.js';
import { LLM_PROVIDER_DEFAULTS } from '../../../common/configData.js';

export class GeminiProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = config.provider;
        const defaultBaseURL = LLM_PROVIDER_DEFAULTS[this.providerName]?.baseURL;
        this.apiBaseUrl = config.apiBaseUrl || defaultBaseURL;

        if (!this.apiBaseUrl) {
            throw new Error(`Base URL for provider '${this.providerName}' is not defined.`);
        }
    }

    /**
     * Transform OpenAI-style messages to Gemini format
     */
    async _transformMessages(messages) {
        const contents = [];
        
        for (const message of messages) {
            if (message.role === 'system') {
                // Gemini doesn't have system role, prepend to first user message
                continue;
            }
            
            const parts = [];
            
            if (typeof message.content === 'string') {
                parts.push({ text: message.content });
            } else if (Array.isArray(message.content)) {
                for (const part of message.content) {
                    if (part.type === 'text') {
                        parts.push({ text: part.text });
                    } else if (part.type === 'image_url') {
                        const source = part.image_url.url;
                        const { mimeType, base64 } = await processAttachment(source);
                        parts.push({
                            inline_data: {
                                mime_type: mimeType,
                                data: base64
                            }
                        });
                    }
                }
            }
            
            contents.push({
                role: message.role === 'assistant' ? 'model' : 'user',
                parts
            });
        }
        
        // Prepend system message to first user message if exists
        const systemMessage = messages.find(m => m.role === 'system');
        if (systemMessage && contents.length > 0) {
            const firstUserIdx = contents.findIndex(c => c.role === 'user');
            if (firstUserIdx !== -1) {
                contents[firstUserIdx].parts.unshift({
                    text: `System Instructions: ${systemMessage.content}\n\n`
                });
            }
        }
        
        return contents;
    }

    _buildURL(model, stream = false) {
        const method = stream ? 'streamGenerateContent' : 'generateContent';
        return `${this.apiBaseUrl}/${model}:${method}?key=${this.apiKey}`;
    }

    async _fetchAPI(url, body, signal) {
        const response = await fetch(url, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(body),
            signal,
        });

        if (!response.ok) {
            const errorData = await response.json().catch(() => ({ 
                error: { message: response.statusText } 
            }));
            throw new LLMError(errorData.error?.message || 'Gemini API error', {
                cause: errorData,
                provider: this.providerName,
                statusCode: response.status,
            });
        }
        return response;
    }

    async create(params, signal) {
        const contents = await this._transformMessages(params.messages);
        const model = params.model || this.model;
        
        const body = {
            contents,
            generationConfig: {
                temperature: params.options?.temperature,
                maxOutputTokens: params.options?.max_tokens,
                topP: params.options?.top_p,
            }
        };
        
        // Remove undefined values
        Object.keys(body.generationConfig).forEach(key => 
            body.generationConfig[key] === undefined && delete body.generationConfig[key]
        );
        
        const url = this._buildURL(model, false);
        const response = await this._fetchAPI(url, body, signal);
        const data = await response.json();
        
        // Transform Gemini response to OpenAI format
        const candidate = data.candidates?.[0];
        return {
            choices: [{
                message: {
                    role: 'assistant',
                    content: candidate?.content?.parts?.map(p => p.text).join('') || ''
                },
                finish_reason: candidate?.finishReason?.toLowerCase()
            }],
            usage: {
                prompt_tokens: data.usageMetadata?.promptTokenCount,
                completion_tokens: data.usageMetadata?.candidatesTokenCount,
                total_tokens: data.usageMetadata?.totalTokenCount
            },
            model: model
        };
    }

    async* stream(params, signal) {
        const contents = await this._transformMessages(params.messages);
        const model = params.model || this.model;
        
        const body = {
            contents,
            generationConfig: {
                temperature: params.options?.temperature,
                maxOutputTokens: params.options?.max_tokens,
                topP: params.options?.top_p,
            }
        };
        
        Object.keys(body.generationConfig).forEach(key => 
            body.generationConfig[key] === undefined && delete body.generationConfig[key]
        );
        
        const url = this._buildURL(model, true);
        const response = await this._fetchAPI(url, body, signal);
        
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop();

            for (const line of lines) {
                if (line.trim() === '') continue;
                
                try {
                    const parsedData = JSON.parse(line);
                    const candidate = parsedData.candidates?.[0];
                    const content = candidate?.content?.parts?.map(p => p.text).join('') || '';
                    
                    if (content) {
                        yield {
                            choices: [{
                                delta: { content },
                                finish_reason: candidate?.finishReason?.toLowerCase() || null
                            }]
                        };
                    }
                } catch (e) {
                    console.error('Failed to parse Gemini stream chunk:', line);
                }
            }
        }
    }
}
